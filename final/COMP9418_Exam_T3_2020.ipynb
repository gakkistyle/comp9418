{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3-2020 Exam\n",
    "\n",
    "**COMP9418 - Advanced Topics in Statistical Machine Learning**\n",
    "\n",
    "**University of New South Wales**\n",
    "\n",
    "**7th December, 2020**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, please read and acknowledge the following (double-click on this cell and put an `X` between the brackets `[X]`):\n",
    "    \n",
    "- [X] I acknowledge that I will complete all of the work I submit for this exam without assistance from anyone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "1. This exam will last for 24 hours, starting 7/12/2020 at 00:00:01 AEST and ending 7/12/2020 at 23:59:59 AEST.\n",
    "2. Questions will be answered from 9 am to 9 pm AEST. Questions should be posted in the [WebCMS forum](https://webcms3.cse.unsw.edu.au/COMP9418/20T3/forums/) or sent by email to cs9418@cse.unsw.edu.au.\n",
    "3. You must provide all answers in this Jupyter notebook. \n",
    "4. You must use the cells provided to answer the questions. Use markdown cells for textual answers and code cells for programming answers.\n",
    "5. Submit this exam by give (command line or WebCMS) before the deadline. If WebCMS submission is slow, or if you are submitting in the last hour, please submit using the give command on the CSE servers (via VLAB or ssh).\n",
    "The appropriate command is ```give cs9418 exam *.ipynb```. We will not accept late submissions.\n",
    "6. The exam will have three parts: Multiple choice questions (20%); Questions that require a textual answer (50%); and, programming questions in Python (30%).\n",
    "7. This exam is an open book exam. You are permitted to access papers and books as well as the course materials, including slides and solved tutorials. Please, in case of doubt, read the [UNSW guidance on open book exams](https://student.unsw.edu.au/open-book-and-take-home-exams).\n",
    "8. You are not permitted to communicate (email, phone, message, talk, etc.) with anyone during the exam, except COMP9418 staff via email or forum.\n",
    "9. Do not communicate your exam answers after you finish your exam. Some students may have extended time to complete the exam.\n",
    "10. Do not place your exam work in any location accessible to any other person, such as  Dropbox and Github.\n",
    "11. Ensure that no other person in your household can access your work.\n",
    "12. Do not disclose your zpass to any other person. If you have revealed your zpass, you should change it immediately.\n",
    "13. We will refer deliberate violations of exam conditions to Student Integrity as serious misconduct. \n",
    "14. This exam has nine questions. The total number of marks is 100.\n",
    "15. **Type your student number and name on the next cell.**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student identification\n",
    "\n",
    "**Name:** Qiwen Zheng\n",
    "\n",
    "**Student ID:** z5240149\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [20 marks]\n",
    "\n",
    "Part 1 is composed of four multiple-choice questions of five marks each. To answer each question, double-click the cell with the alternatives and write an `X` between the `[ ]` of the chosen option.\n",
    "\n",
    "This is an example before inserting `X`\n",
    "\n",
    "1. [ ] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "This is an example after inserting `X`\n",
    "\n",
    "1. [X] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "For all four questions, choose only one among the alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1 [5 marks]\n",
    "\n",
    "Log-probabilities is the term we use to denote the value of $\\log p$ for a probability $p$. The main use of log-probabilities is to avoid underflows that may occur when we multiply a large number of probabilities together. In this case, the multiplication $\\prod_i p_i$ becomes a summation since $\\sum_i \\log p_i = \\log \\prod_i p_i$. We also note that the maximisation is a valid operation for log probabilities since $\\log \\max(p_1,...,p_n) = \\max(\\log p_1, ..., \\log p_n)$. The use of log-probabilities is a standard technique in the implementation of several algorithms such as Viterbi and MPE-VE. Still, it is not usually used in others such as Variable Elimination (VE) and MAP-VE. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] The Viterbi and MPE-VE are algorithms that involve the multiplication of a large number of probabilities and, therefore, justify the use of log-probabilities.\n",
    "2. [X] The Viterbi and MPE-VE algorithms require multiplication and maximisation of probabilities. However, VE and MAP-VE may also involve sums of probabilities, which is an operation without a counterpart in the log-probability representation.\n",
    "3. [ ] The Viterbi and MPE-VE algorithms are algorithms in which we are interested in the assignment with maximum probability, instead of the value of the probabilities. Therefore, in these algorithms, we can use log-probabilities since we do not need to report the probabilities.\n",
    "4. [ ] The Viterbi is a particular case of MPE-VE. In turn, MPE-VE is a specific case of MAP-VE. All these algorithms are specialisations of the VE algorithm. Therefore, these are essentially the same algorithm. It makes no difference if we use log-probabilities or probabilities in all of them.\n",
    "5. [ ] None of the above alternatives is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2 [5 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course, we covered a multitude of inference algorithms, from the most straightforward Variable Elimination (VE) to more sophisticated ones such as Iterative Joingraph Propagation (IJGP) and Gibbs samplings. Select the correct alternative regarding the inference algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [X] In the course, we covered two exact inference algorithms: variable elimination and the jointree algorithm. The efficiency of these algorithms is tightly coupled. If the best VE order has width $w$, then it is guaranteed the best jointree will also have width $w$. Similarly, if the width of the best jointree is $w$, then the best VE order has also width $w$.\n",
    "2. [ ] VE is a simple algorithm, but its complexity is exponential for both best and worst cases. Therefore, this algorithm does not scale to large networks. With an extensive network with hundreds of nodes, it is guaranteed VE will not provide answers in feasible time and we will need to rely on more efficient algorithms, such as approximate inference with sampling.\n",
    "3. [ ] We can use the Chebyshev and Hoeffding bounds to compute the number of samples necessary for inference with sampling algorithms. Those bounds are accurate independently of the sampling algorithm: Forward, Rejection, Gibbs sampling, as well as Likelihood Weighting.\n",
    "4. [ ] Joingraphs are similar to Jointrees but with relaxed constraints. The inference algorithms for both structures are also very similar. While the inference algorithm for Jointrees converges in a single iteration, the IJGP is guaranteed to converge in one or more iterations. \n",
    "5. [ ] None of the above alternatives is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3 [5 marks]\n",
    "\n",
    "In Lecture 16, we studied approaches to learn network structures from data. Choose the **incorrect** alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] Learning tree structures is more straightforward than learning graph structures because trees have a fixed number of edges.\n",
    "2. [ ] A limitation of the tree structure learning methods is that they can infer the presence of an edge but do not infer its direction.\n",
    "3. [X] Learning DAG structures is always better than learning tree structures. The addition of new edges to tree structures will transform the tree into a DAG and will never increase the log-likelihood of the network.\n",
    "4. [ ] Model complexity is a technique to avoid overfitting in DAG structure learning. Model complexity is necessary because the addition of an edge never decreases the log-likelihood of the resulting structure.\n",
    "5. [ ] Optimal search, as the name suggests, can identify the optimal parent set for a given total order of variables. However, the time complexity is exponential. Therefore, the importance of relying on pruning techniques to reduce running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 4 [5 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 4, we discussed the concept of I-MAP, D-MAP and P-MAP for Bayesian networks. We can extend this concept to Markov networks, Jointrees and Joingraphs. Also, in Lecture 9, we discussed that our graphical models could be understood as languages to represent independencies. Now, suppose we have a probability distribution $P$ over five variables $A,B,C,D$ and $E$ such that $P(A,B,C,D,E) = \\phi(A,B,C)\\phi(B,E)\\phi(E,D)\\phi(C,D)$. Which probabilistic graphical model is a language able to provide a graph that is a P-MAP for the probability distribution $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] A Bayesian Network. \n",
    "2. [ ] A Markov Network and a Bayesian Network.\n",
    "3. [ ] A Joingraph and a Markov Network.\n",
    "4. [ ] A Joingraph, Markov Network and Bayesian Network. \n",
    "5. [ ] A Joingraph, Jointree, Markov Network and Bayesian Network.\n",
    "6. [X] A Markov Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 2 [50 marks]\n",
    "\n",
    "Part 2 is composed of three open questions. To answer each question, edit the markdown cell after the question statement and insert your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 5 [15 marks]\n",
    "\n",
    "As a data scientist, you visit a potential client, a doctor, that poses the following problem to you. He wants to use Machine Learning to improve his understanding of cancer patients. All patients in the database were diagnosed with some variation of the disease. The database has information about the patients (such as age, gender, etc.), medical history (previous and existing conditions) and the disease (cancer type, tumour size, etc.). The doctor has three main requirements: \n",
    "\n",
    "* They want to understand and validate the model; \n",
    "* They want the model to incorporate their knowledge about the domain, such as \"lung cancer has a high prevalence for smokers\", but the model should also learn other relationships from data; \n",
    "* They have not a single query they are interested in; they want to probe the model to find \"interesting things\".\n",
    "\n",
    "Answer:\n",
    "\n",
    "1. [**5 Marks**] What model would you recommend: generative or discriminative. Briefly explain why.\n",
    "2. [**5 Marks**] Briefly compare the suitability of two explainable models for this problem: decision trees and Bayesian networks.\n",
    "3. [**5 Marks**] In the case of Bayesian networks, how would you deal with the requirement of incorporating existing human knowledge in the model? Respond considering the model structure and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1\n",
    "\n",
    "Generative model. \n",
    "Because generative model models P(X) being X a set of variable correspond to graph nodes and thet can be used to answer any queries that involve variables in X. Whereas discriminative models approximate P(Y|X) that can only answer queries that involve estimating the probability of Y given X. For this particular task, they have not a single query they are interested in, so generative model provides more convenient method to derive probability of other variables, not only the probability of having cancer, but also the information of other \"interesting things\" such as age, tumour size, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "For this problem, Bayesian network can be more suitable. \n",
    "\n",
    "Since Bayesian network is generative model, it can be used to answer any queries( prior marginals, posterior marginals), so it meet the requirements that they have not a single query they are interested in. On contrast, decision trees is discriminative model, generally the leaf node assigns classification. For this task, we need to create different decision trees to deal with different classification problems, which is quite inefficient.\n",
    "\n",
    "In addition, Bayesian network graph structure can be built by doctor domain experience, such as age and smoker can be direct causes for cancer.So it is easiear to understand and incorporate their knowledge about the domain. Whereas decision trees method needs to compute entropy to decide which step to split which attribute, which is less intuitive.\n",
    "\n",
    "Moreover, as generative model, Bayesian network requires less data to gain better accuracy performance. Also it is useful when missing data is present.\n",
    "\n",
    "Therefore, Bayesian network is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 3\n",
    "\n",
    "Considering the model structure, the requirement of incorporating existing human knowledge in the model can be met by causal perceptions proposed by expert (doctor). The doctor can engage in building up the network structure, instructing us the causality to motivate the interpretation. For example, doctor can propose that age has a direct influence on probability of having cancer.\n",
    "\n",
    "When it comes to the model parameters, we need to quantify the dependencies between nodes and their parents. This process is much easier to accomplish by an expert if the DAG corresponds to causal perceptions. The specification of CPTs for the developed network structure is usally obtained from the expert who supplies the information. This time, the construction of CPTs relys on the data provided by doctors and patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 6 [20 marks]\n",
    "\n",
    "The drive in golf is the first shot in playing a hole. If you drive with a 3-wood (a particular type of golf club), there is a 2% risk of a miss (hitting the ball at the wrong angle, so it goes in the wrong direction), and 1/4 of the drives have a length of 180 m, 1/2 are 200 m, and 1/4 have a length of 220 m. You may also use a driver (another type of golf club). This will increase the length by 20 m, but you will also have three times as high a risk of a miss. Both wind and the slope of the hole may affect the result of the drive. The presence of wind doubles the risk of a miss, and the length is affected by 20 m (longer if the wind is from behind and shorter from the front). A downhill slope yields 20 m longer drives and an uphill slope decreases the length of the drive by 20 m. What is the probability of a miss given a shot greater or equal to 260 m?\n",
    "\n",
    "1. [**5 Marks**] Show a Bayesian network structure (graph) for this problem. **Briefly** explain your network. You will have to make reasonable assumptions when constructing your model.\n",
    "3. [**5 Marks**] Provide a query that solves this problem.\n",
    "4. [**10 Marks**] Answer the query by solving this as a programming exercise, use the tutorial code to make a program that computes the query. If you do not have information about a certain variable, assume it has a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer for 1 - Bayesian network structure\n",
    "\n",
    "# Define your graph here\n",
    "graph = {\n",
    "    'club': ['risk','length'],\n",
    "    'wind': ['risk','length'],\n",
    "    'slope': ['length'],\n",
    "    'length':[],\n",
    "    'risk': ['miss'],\n",
    "    'miss':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"149pt\" height=\"265pt\"\n",
       " viewBox=\"0.00 0.00 148.91 265.27\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 261.27)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-261.27 144.91,-261.27 144.91,4 -4,4\"/>\n",
       "<!-- club -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>club</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-123.62\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-119.92\" font-family=\"Times,serif\" font-size=\"14.00\">club</text>\n",
       "</g>\n",
       "<!-- length -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"75.68\" cy=\"-79.61\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.68\" y=\"-75.91\" font-family=\"Times,serif\" font-size=\"14.00\">length</text>\n",
       "</g>\n",
       "<!-- club&#45;&gt;length -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>club&#45;&gt;length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.21,-108.96C45.64,-106.76 48.2,-104.45 50.76,-102.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"53.3,-104.56 58.37,-95.26 48.61,-99.37 53.3,-104.56\"/>\n",
       "</g>\n",
       "<!-- risk -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>risk</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63.72\" cy=\"-178.05\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.72\" y=\"-174.35\" font-family=\"Times,serif\" font-size=\"14.00\">risk</text>\n",
       "</g>\n",
       "<!-- club&#45;&gt;risk -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>club&#45;&gt;risk</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M38.22,-140.24C40.92,-144.25 43.87,-148.63 46.77,-152.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.08,-155.19 52.57,-161.52 49.88,-151.28 44.08,-155.19\"/>\n",
       "</g>\n",
       "<!-- wind -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>wind</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"112.31\" cy=\"-134.04\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"112.31\" y=\"-130.34\" font-family=\"Times,serif\" font-size=\"14.00\">wind</text>\n",
       "</g>\n",
       "<!-- wind&#45;&gt;length -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>wind&#45;&gt;length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M101.12,-117.41C98.47,-113.48 95.59,-109.19 92.74,-104.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.54,-102.85 87.05,-96.51 89.73,-106.76 95.54,-102.85\"/>\n",
       "</g>\n",
       "<!-- wind&#45;&gt;risk -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>wind&#45;&gt;risk</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.86,-148.94C93.11,-151.43 90.21,-154.06 87.32,-156.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84.89,-154.16 79.82,-163.46 89.59,-159.34 84.89,-154.16\"/>\n",
       "</g>\n",
       "<!-- slope -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>slope</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"82.07\" cy=\"-18\" rx=\"29.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.07\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">slope</text>\n",
       "</g>\n",
       "<!-- slope&#45;&gt;length -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>slope&#45;&gt;length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M80.19,-36.15C79.7,-40.87 79.16,-46.07 78.63,-51.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.12,-51.07 77.57,-61.38 82.08,-51.79 75.12,-51.07\"/>\n",
       "</g>\n",
       "<!-- miss -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>miss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"54.28\" cy=\"-239.27\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54.28\" y=\"-235.57\" font-family=\"Times,serif\" font-size=\"14.00\">miss</text>\n",
       "</g>\n",
       "<!-- risk&#45;&gt;miss -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>risk&#45;&gt;miss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M60.94,-196.09C60.21,-200.78 59.42,-205.94 58.63,-211.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.14,-210.73 57.07,-221.15 62.06,-211.8 55.14,-210.73\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7ff92e67f630>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not modify this cell, it simply plots the graph above\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(engine=\"neato\", comment='Direct graph example')\n",
    "dot.attr(overlap=\"false\", splines=\"true\")\n",
    "\n",
    "for v in graph.keys():\n",
    "    dot.node(v)\n",
    "\n",
    "for v in graph.keys():\n",
    "    for w in graph[v]:\n",
    "        dot.edge(v, w)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1 - Brief explanation\n",
    "\n",
    "`club` represents the golf club type( has the value of '3-wood' and 'driver'), `wind` is has the outcome of 'no', 'behind' and 'front'. Both `wind` and `club` has direct influnce on rist rate and length. `slope` only has influence on length, with outcome of 'downhill' and 'uphill', I make assumption here that there is no 'flat' outcome since it is not mentioned in the spec. `length` has multiple values(range from 140 to 280, with step 20), but this time only focus on the value below 260 and greater or equal 260, so set its outcome as 'lt260' and 'ge260'. `risk` has the value of '2%', '4%', '6%' and '12%', it has direct influence on the final result, which is `miss`, having outcome of 'yes' and 'no'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "$P(miss | length = ge260)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| miss   | length   |    Pr |\n",
      "|--------+----------+-------|\n",
      "| yes    | ge260    | 0.092 |\n",
      "| no     | ge260    | 0.908 |\n"
     ]
    }
   ],
   "source": [
    "# Your answer for 3\n",
    "\n",
    "from collections import OrderedDict as odict\n",
    "from itertools import product, combinations, permutations\n",
    "from tabulate import tabulate\n",
    "\n",
    "outcomeSpace = dict(\n",
    "    club=('3-wood','driver'),\n",
    "    wind=('no','behind','front'),\n",
    "    slope=('downhill','uphill'),\n",
    "    length=('lt260','ge260'),\n",
    "    risk=('2%','4%','6%','12%'),\n",
    "    miss=('yes','no')\n",
    ")\n",
    "\n",
    "factors = {\n",
    "    'club': {\n",
    "        'dom': ('club',), \n",
    "        'table': odict([\n",
    "            (('3-wood',), 0.5),\n",
    "            (('driver',), 0.5),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'wind': {\n",
    "        'dom': ('wind',), \n",
    "        'table': odict([\n",
    "            (('no',), 1/3),\n",
    "            (('behind',), 1/3),\n",
    "            (('front',), 1/3),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'slope': {\n",
    "        'dom': ('slope',), \n",
    "        'table': odict([\n",
    "            (('downhill',), 0.5),\n",
    "            (('uphill',), 0.5),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'length': {\n",
    "        'dom': ('club', 'wind', 'slope', 'length'), \n",
    "        'table': odict([\n",
    "            (('3-wood', 'no', 'downhill', 'lt260'), 1),\n",
    "            (('3-wood', 'no', 'downhill', 'ge260'), 0),\n",
    "            (('3-wood', 'no', 'uphill', 'lt260'), 1),\n",
    "            (('3-wood', 'no', 'uphill', 'ge260'), 0),\n",
    "            (('3-wood', 'behind', 'downhill', 'lt260'), 0.75),\n",
    "            (('3-wood', 'behind', 'downhill', 'ge260'), 0.25),\n",
    "            (('3-wood', 'behind', 'uphill', 'lt260'), 1),\n",
    "            (('3-wood', 'behind', 'uphill', 'ge260'), 0),\n",
    "            (('3-wood', 'front', 'downhill', 'lt260'), 1),\n",
    "            (('3-wood', 'front', 'downhill', 'ge260'), 0),\n",
    "            (('3-wood', 'front', 'uphill', 'lt260'), 1),\n",
    "            (('3-wood', 'front', 'uphill', 'ge260'), 0),\n",
    "            (('driver', 'no', 'downhill', 'lt260'), 0.75),\n",
    "            (('driver', 'no', 'downhill', 'ge260'), 0.25),\n",
    "            (('driver', 'no', 'uphill', 'lt260'), 1),\n",
    "            (('driver', 'no', 'uphill', 'ge260'), 0),\n",
    "            (('driver', 'behind', 'downhill', 'lt260'), 0.25),\n",
    "            (('driver', 'behind', 'downhill', 'ge260'), 0.75),\n",
    "            (('driver', 'behind', 'uphill', 'lt260'), 1),\n",
    "            (('driver', 'behind', 'uphill', 'ge260'), 0),\n",
    "            (('driver', 'front', 'downhill', 'lt260'), 1),\n",
    "            (('driver', 'front', 'downhill', 'ge260'), 0),\n",
    "            (('driver', 'front', 'uphill', 'lt260'), 1),\n",
    "            (('driver', 'front', 'uphill', 'ge260'), 0),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'risk': {\n",
    "        'dom': ('club', 'wind', 'risk'), \n",
    "        'table': odict([\n",
    "            (('3-wood', 'no', '2%'), 1),\n",
    "            (('3-wood', 'no', '4%'), 0),\n",
    "            (('3-wood', 'no', '6%'), 0),\n",
    "            (('3-wood', 'no', '12%'), 0),\n",
    "            (('3-wood', 'behind', '2%'), 0),\n",
    "            (('3-wood', 'behind', '4%'), 1),\n",
    "            (('3-wood', 'behind', '6%'), 0),\n",
    "            (('3-wood', 'behind', '12%'), 0),\n",
    "            (('3-wood', 'front', '2%'), 0),\n",
    "            (('3-wood', 'front', '4%'), 1),\n",
    "            (('3-wood', 'front', '6%'), 0),\n",
    "            (('3-wood', 'front', '12%'), 0),\n",
    "            (('driver', 'no', '2%'), 0),\n",
    "            (('driver', 'no', '4%'), 0),\n",
    "            (('driver', 'no', '6%'), 1),\n",
    "            (('driver', 'no', '12%'), 0),\n",
    "            (('driver', 'behind', '2%'), 0),\n",
    "            (('driver', 'behind', '4%'), 0),\n",
    "            (('driver', 'behind', '6%'), 0),\n",
    "            (('driver', 'behind', '12%'), 1),\n",
    "            (('driver', 'front', '2%'), 0),\n",
    "            (('driver', 'front', '4%'), 0),\n",
    "            (('driver', 'front', '6%'), 0),\n",
    "            (('driver', 'front', '12%'), 1),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'miss':{\n",
    "        'dom' : ('risk', 'miss'),\n",
    "        'table' : odict([\n",
    "            (('2%','yes'),0.02),\n",
    "            (('2%','no'),0.98),\n",
    "            (('4%','yes'),0.04),\n",
    "            (('4%','no'),0.96),\n",
    "            (('6%','yes'),0.06),\n",
    "            (('6%','no'),0.94),\n",
    "            (('12%','yes'),0.12),\n",
    "            (('12%','no'),0.88),\n",
    "        ])\n",
    "    }\n",
    "       \n",
    "}\n",
    "\n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "\n",
    "    return factor['table'][entry]     # insert your code here, 1 line   \n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). 1 line\n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. 1 line\n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. 1 line\n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "            \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. 1 line\n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s. 1 line\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    \n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
    "    table = list()\n",
    "    \n",
    "    # Here is where the magic happens. The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = tuple((entryDict[var] for var in f1['dom']))\n",
    "        f2_entry = tuple((entryDict[var] for var in f2['dom']))\n",
    "                \n",
    "        # Insert your code here\n",
    "        p1 = prob(f1, *f1_entry)           # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry \n",
    "        p2 = prob(f2, *f2_entry)           # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry \n",
    "        \n",
    "     \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "# Answer\n",
    "\n",
    "def VE(factors, order, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors, each factor is a dictionary of domain and probability values,\n",
    "    `order`, a list of variable names specifying an elimination order,\n",
    "    `outcomeSpace`, a dictionary with variable names and respective domains.\n",
    "    Returns a dictionary with non-eliminated factors\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of factors, so we can freely modify it without distroying the original dictionary\n",
    "    f = factors.copy()\n",
    "    # We process the factor in elimination order  \n",
    "    for i, var in enumerate(order):\n",
    "        # This is the domain of the new factor. We use sets as it is handy to eliminate duplicate variables\n",
    "        newFactorDom = set()\n",
    "        # This is a list of factors that will be removed from f because they were joined with other factors\n",
    "        listFactorsRemove = list()\n",
    "        # This is a flag to indicate if we are processing the first factor\n",
    "        first = True\n",
    "        # Lets iterate over all factors\n",
    "        for f_id in f.keys():\n",
    "            # and select the ones that have the variable to be eliminated\n",
    "            if var in f[f_id]['dom']:        \n",
    "                if first:\n",
    "                    # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "                    fx = f[f_id]\n",
    "                    first = False\n",
    "                else:\n",
    "                    # Join fx and f[f_id] and save the result in fx\n",
    "                    fx = join(fx, f[f_id], outcomeSpace)\n",
    "                    #printFactor(fx)\n",
    "                # f_id was joined, so we will need to eliminate it from f later. Let's save that factor id for future removal\n",
    "                listFactorsRemove.append(f_id)\n",
    "        # Now, we need to remove var from the domain of the new factor doing a marginalization              \n",
    "        fx = marginalize(fx, var, outcomeSpace)\n",
    "        #printFactor(fx)\n",
    "    \n",
    "        # Now, we remove all factors that we joined. We do it outside the for loop since it modifies the data structure\n",
    "        for f_id in listFactorsRemove:\n",
    "            del f[f_id]\n",
    "        # We will create a new factor with id equal a sequential number and insert it into f, so it can be used in future joins          \n",
    "        f[i] = fx\n",
    "    return f\n",
    "\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        # Convert the tuple to a list to be able to manipulate it\n",
    "        k = list(key)\n",
    "        # Append the probability value to the list with key values\n",
    "        k.append(item)\n",
    "        # Append an entire row to the table\n",
    "        table.append(k)\n",
    "    # dom is used as table header. We need it converted to list\n",
    "    dom = list(f['dom'])\n",
    "    # Append a 'Pr' to indicate the probabity column\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='orgtbl'))\n",
    "    \n",
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      # Make a copy of outcomeSpace with a copy to method copy(). 1 line\n",
    "    newOutcomeSpace[var] = (e,)                # Replace the domain of variable var with a tuple with a single element e. 1 line\n",
    "    return newOutcomeSpace\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "\n",
    "def query(factors, order, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors\n",
    "    `order`, a list with variable elimination order\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor with P(Q, e) or P(Q|e)\n",
    "    \"\"\"     \n",
    "        \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    outSpace = outcomeSpace.copy()\n",
    "        \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)    \n",
    "    \n",
    "    for q_var in q_vars:\n",
    "        order.remove(q_var)\n",
    " \n",
    "    f = VE(factors, order, outSpace)\n",
    "        \n",
    "    first = True\n",
    "    for f_id in f.keys():\n",
    "        if first:\n",
    "            # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "            fx = f[f_id]\n",
    "            first = False\n",
    "        else:\n",
    "            # Join fx and f[f_id] and save the result in fx\n",
    "            fx = join(fx, f[f_id], outSpace)    \n",
    "    \n",
    "    #printFactor(fx)\n",
    "    return normalize(fx)\n",
    "\n",
    "printFactor(query(factors, ['miss','risk' , 'slope', 'wind', 'club'], outcomeSpace, ['miss'], length = 'ge260'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 7 [15 marks]\n",
    "\n",
    "In Lecture 15, we discussed techniques to learn parameters from data, and it became evident that learning in the presence of missing data is significantly more expensive than with complete data. In this question, we ask you to analyse the time complexity of the EM algorithm in more detail.\n",
    "\n",
    "1. [**10 marks**] The Expectation Maximisation (EM) algorithm requires inference on the Bayesian network. Explain how the jointree algorithm can help to improve the running time of the EM algorithm. In particular, queries of the form $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ involve setting evidence $\\textbf{d}_i$ for every training example $i$. How does it impact the performance of the jointree algorithm in terms of new messages that need to be exchanged?\n",
    "2. [**5 marks**] It is a common practice to use *random restarts* with EM since the algorithm is sensitive to the initial parameter estimate $\\theta^0$. The idea is to run the algorithm multiple times and return the best estimate. Explain how you would implement random restarts with EM. How would you initialise the algorithm, and how would you select the best estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1\n",
    "Each iteration the algorithm computes the probability of each instantiation xu given each case di as evidence, all these computations correspond to posterior marginals over network families. Hence, we want to use jointree algorithm that can compute family marginals efficiently.\n",
    "\n",
    "For each iteration of parameters set $\\theta^k$ of the network, we calculate $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ for each family instantiation, which requires inference on network (G, $\\theta^k$) that can be effienctly computed by Jointree algorithm. Since $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ = $\\eta$ $\\lambda_{d_i}$$\\phi(XU)$$\\pi_j(M_{ji})$, where $\\eta$ is a normalization number, $\\lambda_{d_i}$ is the evidence \n",
    "set by dataset, $\\phi_{j}(XU)$ is the factor multiplication within one cluster that corresponds to the family instantiation, $\\pi_j(M_{ji})$ is the multiplication of comming in messages from other clusters.\n",
    "\n",
    "The advantage of using jointree is that for each family instantiation xu, it can build a cluster for that xu and thus the full factor table of the family instantiation is obtained, which is $\\phi(XU)$ mentioned above.By one propogation it can generate the $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ for each instantiation of xu. The normal way, however, has to compute the probability given the evidence once and once again for different instantiation of xu.\n",
    "\n",
    "In particular, for different training example, the evidence term is changed, so the message along the paths also needs changes. Each family cluster needs reset of evidence, and the message $\\pi_j(M_{ji})$ towards family xu is different for different sets of evidence, so they need to be recomputed. Therefore, the new messages that need to be exchanged each time can slow down the jointree algorithm speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "Each time we initialize the parameter set  $\\theta^0$ of the whole CPTs randomly and record the initial parameters to avoid the repeated occurance. Then we use the inial parameters to do EM algorithm until its convergence. We record the set of parameters and the likelihood value L($\\theta$;D). This is one pass. We repeatedly do the above algorithm, each time compare the convergent likelihood value with the maximum one, if bigger than maximum, we update the maximum and take down the set of parameters. The number of loops can be determined by the user. Normally larger network needs more time of the loop above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 3 [30 marks]\n",
    "\n",
    "Part 3 is composed of two programming questions of 15 marks each. Use the code cell after each question statement to enter your answer. You can use the code of the tutorials in your answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 8 [15 marks]\n",
    "\n",
    "Lecture 9 presented the concept of **separation**, an independence test for Markov networks. The idea is similar, but simpler than d-separation since Markov networks do not have \"convergent valves\". The test can be summarized in a single sentence:\n",
    "\n",
    "Let $\\textbf{X}$, $\\textbf{Y}$, and $\\textbf{Z}$ be three disjoint sets of nodes in a graph $G$. We say that $\\textbf{X}$ is separated from $\\textbf{Y}$ given $\\textbf{Z}$, written $sep_G(\\textbf{X},\\textbf{Z},\\textbf{Y})$, if and only if every path between a node in $\\textbf{X}$ and a node in $\\textbf{Y}$ pass through a node in $\\textbf{Z}$.\n",
    "\n",
    "An efficient separation test can be implemented by pruning the edges of $\\textbf{Z}$ and testing for connectivity between nodes in $\\textbf{X}$ and $\\textbf{Y}$.\n",
    "\n",
    "Implement an efficient separation test for Markov networks. The function `separation(G, X, Z, Y)` returns true if $\\textbf{X}$ is separated of $\\textbf{Y}$ given $\\textbf{Z}$ in the graph $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our answer for Question 8 here\n",
    "def delete_leaf(G,node):\n",
    "    for start in G.keys():\n",
    "        if node in G[start]:\n",
    "            G[start].remove(node)\n",
    "    G.pop(node)\n",
    "\n",
    "def make_directed(G):\n",
    "    for from_v in G.keys():\n",
    "        for to_v in G[from_v]:\n",
    "            if from_v not in G[to_v]:\n",
    "                G[to_v].append(from_v)\n",
    "            \n",
    "def dfs_search(G,colour,set_Y,node):\n",
    "    colour[node] = 'grey'\n",
    "    if node in set_Y:\n",
    "        return False\n",
    "    \n",
    "    for v in G[node]:\n",
    "        if colour[v] == 'white':\n",
    "            if dfs_search(G,colour,set_Y,v) == False:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def separation(G, X, Z, Y):\n",
    "    \n",
    "    G_copy = dict((v, []) for v in G)\n",
    "    for v in G:\n",
    "        G_copy[v] = G[v][:]\n",
    "            \n",
    "    #delete any outgoing edge from Z\n",
    "    for v in Z:\n",
    "        G_copy[v].clear()\n",
    "        for n in G_copy.keys():\n",
    "            if v in G_copy[n]:\n",
    "                G_copy[n].remove(v)\n",
    "            \n",
    "        \n",
    "    #then make the G_copy to be undirected graph\n",
    "    make_directed(G_copy)\n",
    "    \n",
    "    #use dfs search to find whether X are separated from Y\n",
    "    colour = {node: 'white' for node in G_copy.keys()}\n",
    "    for v in X:\n",
    "        #if we find a possible path from node of X to node of Y,return false\n",
    "        if not dfs_search(G_copy,colour,Y,v):\n",
    "            return False         \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Test code\n",
    "\n",
    "graph = {\n",
    "    'A': ['B'],\n",
    "    'B': ['A', 'C', 'D', 'F'],\n",
    "    'C': ['B'],\n",
    "    'D': ['B', 'F'],\n",
    "    'E': ['F'],\n",
    "    'F': ['B', 'D', 'E', 'G'],\n",
    "    'G': ['F'],\n",
    "}\n",
    "\n",
    "print(separation(graph, ['A'], ['D'], ['G']))\n",
    "print(separation(graph, ['A'], ['C'], ['G']))\n",
    "print(separation(graph, ['A'], ['F'], ['G']))\n",
    "print(separation(graph, ['A'], ['D', 'E'], ['G']))\n",
    "print(separation(graph, ['A', 'D'], ['C', 'G'], ['E', 'F']))\n",
    "print(separation(graph, ['A', 'D'], ['F'], ['E', 'G']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 9 [15 marks]\n",
    "\n",
    "In Lecture 16, we discussed the importance of measuring model complexity. A common way to do so is to count the number of independent parameters in a Bayesian network.\n",
    "\n",
    "In this exercise, you will write a function `dimension(G, outcomeSpace)` which takes a DAG `G` and its associated `outcomeSpace`, and returns the dimension of `G`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our answer for Question 8 here\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v, []) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "    return GT\n",
    "\n",
    "def dimension(G, outcomeSpace):\n",
    "    dim = 0\n",
    "    GT = transposeGraph(G)\n",
    "    \n",
    "    for node in GT.keys():\n",
    "        XU = len(outcomeSpace[node]) - 1\n",
    "        for parent in GT[node]:\n",
    "            XU *= len(outcomeSpace[parent])\n",
    "        dim += XU\n",
    "    return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the ICU network is 37\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Test code\n",
    "\n",
    "graph = {\n",
    "    'L': ['S', 'V'],\n",
    "    'H': ['S', 'V'],\n",
    "    'S': ['O'],\n",
    "    'V': ['C', 'O'],\n",
    "    'O': ['B'],\n",
    "    'A': ['T'],\n",
    "    'T': ['B'],\n",
    "    'C': [],\n",
    "    'B': [],\n",
    "}\n",
    "\n",
    "outcomeSpace = dict(\n",
    "    H=(0,1),\n",
    "    L=(0,1),\n",
    "    A=(0,1),\n",
    "    V=(0,1),\n",
    "    S=(0,1),\n",
    "    T=(0,1),\n",
    "    C=(0,1,2),\n",
    "    O=(0,1,2),\n",
    "    B=(0,1,2),\n",
    ")\n",
    "\n",
    "print(\"The dimension of the ICU network is\", dimension(graph, outcomeSpace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "783px",
    "left": "0px",
    "right": "1346.87px",
    "top": "108px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
