{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3-2020 Exam Solution\n",
    "\n",
    "**COMP9418 - Advanced Topics in Statistical Machine Learning**\n",
    "\n",
    "**University of New South Wales**\n",
    "\n",
    "**7th December, 2020**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, please read and acknowledge the following (double-click on this cell and put an `X` between the brackets `[X]`):\n",
    "    \n",
    "- [ ] I acknowledge that I will complete all of the work I submit for this exam without assistance from anyone else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "1. This exam will last for 24 hours, starting 7/12/2020 at 00:00:01 AEST and ending 7/12/2020 at 23:59:59 AEST.\n",
    "2. Questions will be answered from 9 am to 9 pm AEST. Questions should be posted in the [WebCMS forum](https://webcms3.cse.unsw.edu.au/COMP9418/20T3/forums/) or sent by email to cs9418@cse.unsw.edu.au.\n",
    "3. You must provide all answers in this Jupyter notebook. \n",
    "4. You must use the cells provided to answer the questions. Use markdown cells for textual answers and code cells for programming answers.\n",
    "5. Submit this exam by give (command line or WebCMS) before the deadline. If WebCMS submission is slow, or if you are submitting in the last hour, please submit using the give command on the CSE servers (via VLAB or ssh).\n",
    "The appropriate command is ```give cs9418 exam *.ipynb```. We will not accept late submissions.\n",
    "6. The exam will have three parts: Multiple choice questions (20%); Questions that require a textual answer (50%); and, programming questions in Python (30%).\n",
    "7. This exam is an open book exam. You are permitted to access papers and books as well as the course materials, including slides and solved tutorials. Please, in case of doubt, read the [UNSW guidance on open book exams](https://student.unsw.edu.au/open-book-and-take-home-exams).\n",
    "8. You are not permitted to communicate (email, phone, message, talk, etc.) with anyone during the exam, except COMP9418 staff via email or forum.\n",
    "9. Do not communicate your exam answers after you finish your exam. Some students may have extended time to complete the exam.\n",
    "10. Do not place your exam work in any location accessible to any other person, such as  Dropbox and Github.\n",
    "11. Ensure that no other person in your household can access your work.\n",
    "12. Do not disclose your zpass to any other person. If you have revealed your zpass, you should change it immediately.\n",
    "13. We will refer deliberate violations of exam conditions to Student Integrity as serious misconduct. \n",
    "14. This exam has nine questions. The total number of marks is 100.\n",
    "15. **Type your student number and name on the next cell.**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student identification\n",
    "\n",
    "**Name:**\n",
    "\n",
    "**Student ID:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [20 marks]\n",
    "\n",
    "Part 1 is composed of four multiple-choice questions of five marks each. To answer each question, double-click the cell with the alternatives and write an `X` between the `[ ]` of the chosen option.\n",
    "\n",
    "This is an example before inserting `X`\n",
    "\n",
    "1. [ ] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "This is an example after inserting `X`\n",
    "\n",
    "1. [X] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "For all four questions, choose only one among the alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1 [5 marks]\n",
    "\n",
    "Log-probabilities is the term we use to denote the value of $\\log p$ for a probability $p$. The main use of log-probabilities is to avoid underflows that may occur when we multiply a large number of probabilities together. In this case, the multiplication $\\prod_i p_i$ becomes a summation since $\\sum_i \\log p_i = \\log \\prod_i p_i$. We also note that the maximisation is a valid operation for log probabilities since $\\log \\max(p_1,...,p_n) = \\max(\\log p_1, ..., \\log p_n)$. The use of log-probabilities is a standard technique in the implementation of several algorithms such as Viterbi and MPE-VE. Still, it is not usually used in others such as Variable Elimination (VE) and MAP-VE. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] The Viterbi and MPE-VE are algorithms that involve the multiplication of a large number of probabilities and, therefore, justify the use of log-probabilities.\n",
    "2. [X] The Viterbi and MPE-VE algorithms require multiplication and maximisation of probabilities. However, VE and MAP-VE may also involve sums of probabilities, which is an operation without a counterpart in the log-probability representation.\n",
    "3. [ ] The Viterbi and MPE-VE algorithms are algorithms in which we are interested in the assignment with maximum probability, instead of the value of the probabilities. Therefore, in these algorithms, we can use log-probabilities since we do not need to report the probabilities.\n",
    "4. [ ] The Viterbi is a particular case of MPE-VE. In turn, MPE-VE is a specific case of MAP-VE. All these algorithms are specialisations of the VE algorithm. Therefore, these are essentially the same algorithm. It makes no difference if we use log-probabilities or probabilities in all of them.\n",
    "5. [ ] None of the above alternatives is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The marginalisation operation involves summing probabilities. If we use the log-probability representation, we cannot simply sum the log-probabilities since $\\sum_i p_i \\neq \\exp(\\sum_i \\log p_i$). Therefore, the marginalisation operation would require conversion back to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2 [5 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course, we covered a multitude of inference algorithms, from the most straightforward Variable Elimination (VE) to more sophisticated ones such as Iterative Joingraph Propagation (IJGP) and Gibbs samplings. Select the correct alternative regarding the inference algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [X] In the course, we covered two exact inference algorithms: variable elimination and the jointree algorithm. The efficiency of these algorithms is tightly coupled. If the best VE order has width $w$, then it is guaranteed the best jointree will also have width $w$. Similarly, if the width of the best jointree is $w$, then the best VE order has also width $w$.\n",
    "2. [ ] VE is a simple algorithm, but its complexity is exponential for both best and worst cases. Therefore, this algorithm does not scale to large networks. With an extensive network with hundreds of nodes, it is guaranteed VE will not provide answers in feasible time and we will need to rely on more efficient algorithms, such as approximate inference with sampling.\n",
    "3. [ ] We can use the Chebyshev and Hoeffding bounds to compute the number of samples necessary for inference with sampling algorithms. Those bounds are accurate independently of the sampling algorithm: Forward, Rejection, Gibbs sampling, as well as Likelihood Weighting.\n",
    "4. [ ] Joingraphs are similar to Jointrees but with relaxed constraints. The inference algorithms for both structures are also very similar. While the inference algorithm for Jointrees converges in a single iteration, the IJGP is guaranteed to converge in one or more iterations. \n",
    "5. [ ] None of the above alternatives is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "We can convert an elimination order of width $w$ into a jointree with the same width. Also, we can convert a jointree of width $w$ into an elimination order of the same width. Given that the treewidth of a graph is the width of the best elimination order $o^*$, we can convert $o^*$ into a jointree that also has a width equal to the treewidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3 [5 marks]\n",
    "\n",
    "In Lecture 16, we studied approaches to learn network structures from data. Choose the **incorrect** alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] Learning tree structures is more straightforward than learning graph structures because trees have a fixed number of edges.\n",
    "2. [ ] A limitation of the tree structure learning methods is that they can infer the presence of an edge but do not infer its direction.\n",
    "3. [X] Learning DAG structures is always better than learning tree structures. The addition of new edges to tree structures will transform the tree into a DAG and will never decrease the log-likelihood of the network.\n",
    "4. [ ] Model complexity is a technique to avoid overfitting in DAG structure learning. Model complexity is necessary because the addition of an edge never decreases the log-likelihood of the resulting structure.\n",
    "5. [ ] Optimal search, as the name suggests, can identify the optimal parent set for a given total order of variables. However, the time complexity is exponential. Therefore, the importance of relying on pruning techniques to reduce running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "Although DAGs have log-likelihoods equal to or larger than trees, DAG structures are not necessarily better than tree structures. The addition of edges can lead to overfitting and make the models increasingly more complex. Complex models may require more data to learn and tend to be more costly to make an inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 4 [5 marks]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 4, we discussed the concept of I-MAP, D-MAP and P-MAP for Bayesian networks. We can extend this concept to Markov networks, Jointrees and Joingraphs. Also, in Lecture 9, we discussed that our graphical models could be understood as languages to represent independencies. Now, suppose we have a probability distribution $P$ over five variables $A,B,C,D$ and $E$ such that $P(A,B,C,D,E) = \\phi(A,B,C)\\phi(B,E)\\phi(E,D)\\phi(C,D)$. Which probabilistic graphical model is a language able to provide a graph that is a P-MAP for the probability distribution $P$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] A Bayesian Network. \n",
    "2. [ ] A Markov Network and a Bayesian Network.\n",
    "3. [X] A Joingraph and a Markov Network.\n",
    "4. [ ] A Joingraph, Markov Network and Bayesian Network. \n",
    "5. [ ] A Joingraph, Jointree, Markov Network and Bayesian Network.\n",
    "6. [ ] A Markov Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "A Markov network can directly represent this factorisation. From this network, we can derive, using separation, all independency assumptions encoded in the factorisation. They are:\n",
    "\n",
    "$A \\perp D, E | B, C$\n",
    "\n",
    "$A \\perp D, E | B, D$\n",
    "\n",
    "$B \\perp D | C, E$\n",
    "\n",
    "$C \\perp E | B, D$\n",
    "\n",
    "Now, we need to find a Bayesian Network, Joingraph or Jointree that is a P-MAP for these independency assumptions. Remember that to be a P-MAP, the PGM must encode only these assumptions, no more or less.\n",
    "\n",
    "We can find a joingraph that encodes these assumptions, which is not surprising since the joingraph can have the same structure as the Markov network. However, Bayesian networks cannot encode the same assumptions due to the presence of \"convergent structures\". Similarly, the jointree cannot represent the same independencies due to its tree shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 2 [50 marks]\n",
    "\n",
    "Part 2 is composed of three open questions. To answer each question, edit the markdown cell after the question statement and insert your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 5 [15 marks]\n",
    "\n",
    "As a data scientist, you visit a potential client, a doctor, that poses the following problem to you. He wants to use Machine Learning to improve his understanding of cancer patients. All patients in the database were diagnosed with some variation of the disease. The database has information about the patients (such as age, gender, etc.), medical history (previous and existing conditions) and the disease (cancer type, tumour size, etc.). The doctor has three main requirements: \n",
    "\n",
    "* They want to understand and validate the model; \n",
    "* They want the model to incorporate their knowledge about the domain, such as \"lung cancer has a high prevalence for smokers\", but the model should also learn other relationships from data; \n",
    "* They have not a single query they are interested in; they want to probe the model to find \"interesting things\".\n",
    "\n",
    "Answer:\n",
    "\n",
    "1. [**5 Marks**] What model would you recommend: generative or discriminative. Briefly explain why.\n",
    "2. [**5 Marks**] Briefly compare the suitability of two explainable models for this problem: decision trees and Bayesian networks.\n",
    "3. [**5 Marks**] In the case of Bayesian networks, how would you deal with the requirement of incorporating existing human knowledge in the model? Respond considering the model structure and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1\n",
    "\n",
    "The generative models are recommended here since the doctor has not a single query in mind. \"Probing\" the model means posing different queries that would involve modelling $P(\\textbf{X})$ instead just $P(Y|\\textbf{X})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "Decision trees are models that would allow the doctor to \"understand and validate the model\". For every classification, we could provide the sequence of decisions (from the root node to the leaf node) that lead to that particular decision.\n",
    "\n",
    "However, decision trees are classification models, and we cannot easily make them answer different types of queries. The best we can do is to change the class-attribute and generate a new model. However, Bayesian networks would allow for posing queries involving different attributes as query and evidence information.\n",
    "\n",
    "Also, incorporating existing knowledge is not easily done with current implementations of decision trees. The default behaviour is to learn everything from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 3\n",
    "\n",
    "Bayesian networks can be very flexible in incorporating existing knowledge to learn both structure and parameters.\n",
    "\n",
    "For instance, for the Chow-Liu algorithm, we can enforce the MST algorithm to incorporate edges provided by the experts. Similarly, the local search algorithms for DAGs can start with an initial incomplete structure defined by the doctors and add new edges in the search process. Simple modifications can enforce that the search procedure does not discard the provided edges.\n",
    "\n",
    "Also, automatic structure learning is not good to infer the direction of edges. However, we can provide the learned structure to the experts, and they can set the direction of the edges according to some semantics such as causation.\n",
    "\n",
    "Similarly, we can learn the parameters from data or use probabilities provided by the expert. This is not as simple as structure, since a change in the structure will also impact the CPTs, adding or removing variables. The simplest option is to learn the probabilities from data and revise them with the aid of the expert, if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 6 [20 marks]\n",
    "\n",
    "The drive in golf is the first shot in playing a hole. If you drive with a 3-wood (a particular type of golf club), there is a 2% risk of a miss (hitting the ball at the wrong angle, so it goes in the wrong direction), and 1/4 of the drives have a length of 180 m, 1/2 are 200 m, and 1/4 have a length of 220 m. You may also use a driver (another type of golf club). This will increase the length by 20 m, but you will also have three times as high a risk of a miss. Both wind and the slope of the hole may affect the result of the drive. The presence of wind doubles the risk of a miss, and the length is affected by 20 m (longer if the wind is from behind and shorter from the front). A downhill slope yields 20 m longer drives and an uphill slope decreases the length of the drive by 20 m. What is the probability of a miss given a shot greater or equal to 260 m?\n",
    "\n",
    "1. [**5 Marks**] Show a Bayesian network structure (graph) for this problem. **Briefly** explain your network. You will have to make reasonable assumptions when constructing your model.\n",
    "3. [**5 Marks**] Provide a query that solves this problem.\n",
    "4. [**10 Marks**] Answer the query by solving this as a programming exercise, use the tutorial code to make a program that computes the query. If you do not have information about a certain variable, assume it has a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer for 1 - Bayesian network structure\n",
    "\n",
    "# Define your graph here\n",
    "graph = {\n",
    "    'Club': ['Miss', 'Length'],\n",
    "    'Wind': ['Miss', 'Length'],\n",
    "    'Slope': ['Length'],\n",
    "    'Length':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"168pt\" height=\"209pt\"\n",
       " viewBox=\"0.00 0.00 168.11 209.49\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 205.49)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-205.49 164.11,-205.49 164.11,4 -4,4\"/>\n",
       "<!-- Club -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Club</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"32.5\" cy=\"-139.48\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"32.5\" y=\"-135.78\" font-family=\"Times,serif\" font-size=\"14.00\">Club</text>\n",
       "</g>\n",
       "<!-- Length -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Length</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"72.56\" cy=\"-83.21\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"72.56\" y=\"-79.51\" font-family=\"Times,serif\" font-size=\"14.00\">Length</text>\n",
       "</g>\n",
       "<!-- Club&#45;&gt;Length -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Club&#45;&gt;Length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.52,-122.6C47.49,-118.42 50.75,-113.84 53.96,-109.33\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.04,-111.04 59.99,-100.87 51.34,-106.98 57.04,-111.04\"/>\n",
       "</g>\n",
       "<!-- Miss -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Miss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"85.57\" cy=\"-183.49\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.57\" y=\"-179.79\" font-family=\"Times,serif\" font-size=\"14.00\">Miss</text>\n",
       "</g>\n",
       "<!-- Club&#45;&gt;Miss -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Club&#45;&gt;Miss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.76,-154.63C53.63,-157 56.64,-159.5 59.64,-161.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.52,-164.78 67.45,-168.46 61.99,-159.39 57.52,-164.78\"/>\n",
       "</g>\n",
       "<!-- Wind -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Wind</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"125.66\" cy=\"-127.42\" rx=\"34.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.66\" y=\"-123.72\" font-family=\"Times,serif\" font-size=\"14.00\">Wind</text>\n",
       "</g>\n",
       "<!-- Wind&#45;&gt;Length -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Wind&#45;&gt;Length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.09,-111.96C104.79,-110.05 102.4,-108.06 100,-106.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"101.85,-103.05 91.93,-99.34 97.38,-108.43 101.85,-103.05\"/>\n",
       "</g>\n",
       "<!-- Wind&#45;&gt;Miss -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Wind&#45;&gt;Miss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.42,-144.55C110.31,-148.89 106.9,-153.66 103.57,-158.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"100.71,-156.3 97.74,-166.47 106.41,-160.37 100.71,-156.3\"/>\n",
       "</g>\n",
       "<!-- Slope -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Slope</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"64.47\" cy=\"-18\" rx=\"36.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"64.47\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Slope</text>\n",
       "</g>\n",
       "<!-- Slope&#45;&gt;Length -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Slope&#45;&gt;Length</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M66.72,-36.17C67.44,-41.96 68.25,-48.52 69.04,-54.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"65.61,-55.65 70.32,-65.15 72.56,-54.79 65.61,-55.65\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8f88d88fa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not modify this cell, it simply plots the graph above\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(engine=\"neato\", comment='Direct graph example')\n",
    "dot.attr(overlap=\"false\", splines=\"true\")\n",
    "\n",
    "for v in graph.keys():\n",
    "    dot.node(v)\n",
    "\n",
    "for v in graph.keys():\n",
    "    for w in graph[v]:\n",
    "        dot.edge(v, w)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1 - Brief explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of missing a shot is influenced by the club and the presence of wind, while the drive length is influenced by these variables as well as the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "$P(M|L \\geq 260) = \\frac{P(M,L \\geq 260)}{P(L \\geq 260)} = \\frac{P(M,L=260)+P(M,L=280)}{P(L=260)+P(L=280)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| S    | W      |   L | C      |       Pr |\n",
      "|------+--------+-----+--------+----------|\n",
      "| no   | no     | 280 | 3W     | 0        |\n",
      "| no   | no     | 280 | driver | 0        |\n",
      "| no   | front  | 280 | 3W     | 0        |\n",
      "| no   | front  | 280 | driver | 0        |\n",
      "| no   | behind | 280 | 3W     | 0        |\n",
      "| no   | behind | 280 | driver | 0        |\n",
      "| down | no     | 280 | 3W     | 0        |\n",
      "| down | no     | 280 | driver | 0        |\n",
      "| down | front  | 280 | 3W     | 0        |\n",
      "| down | front  | 280 | driver | 0        |\n",
      "| down | behind | 280 | 3W     | 0        |\n",
      "| down | behind | 280 | driver | 0.083325 |\n",
      "| up   | no     | 280 | 3W     | 0        |\n",
      "| up   | no     | 280 | driver | 0        |\n",
      "| up   | front  | 280 | 3W     | 0        |\n",
      "| up   | front  | 280 | driver | 0        |\n",
      "| up   | behind | 280 | 3W     | 0        |\n",
      "| up   | behind | 280 | driver | 0        |\n",
      "\n",
      "| W      |   L | C      |       Pr |\n",
      "|--------+-----+--------+----------|\n",
      "| no     | 280 | 3W     | 0        |\n",
      "| no     | 280 | driver | 0        |\n",
      "| front  | 280 | 3W     | 0        |\n",
      "| front  | 280 | driver | 0        |\n",
      "| behind | 280 | 3W     | 0        |\n",
      "| behind | 280 | driver | 0.083325 |\n",
      "\n",
      "| C      | M   | W      |   Pr |\n",
      "|--------+-----+--------+------|\n",
      "| 3W     | yes | no     | 0.01 |\n",
      "| 3W     | yes | front  | 0.02 |\n",
      "| 3W     | yes | behind | 0.02 |\n",
      "| 3W     | no  | no     | 0.49 |\n",
      "| 3W     | no  | front  | 0.48 |\n",
      "| 3W     | no  | behind | 0.48 |\n",
      "| driver | yes | no     | 0.03 |\n",
      "| driver | yes | front  | 0.06 |\n",
      "| driver | yes | behind | 0.06 |\n",
      "| driver | no  | no     | 0.47 |\n",
      "| driver | no  | front  | 0.44 |\n",
      "| driver | no  | behind | 0.44 |\n",
      "\n",
      "| C      | M   | W      |   L |        Pr |\n",
      "|--------+-----+--------+-----+-----------|\n",
      "| 3W     | yes | no     | 280 | 0         |\n",
      "| 3W     | yes | front  | 280 | 0         |\n",
      "| 3W     | yes | behind | 280 | 0         |\n",
      "| 3W     | no  | no     | 280 | 0         |\n",
      "| 3W     | no  | front  | 280 | 0         |\n",
      "| 3W     | no  | behind | 280 | 0         |\n",
      "| driver | yes | no     | 280 | 0         |\n",
      "| driver | yes | front  | 280 | 0         |\n",
      "| driver | yes | behind | 280 | 0.0049995 |\n",
      "| driver | no  | no     | 280 | 0         |\n",
      "| driver | no  | front  | 280 | 0         |\n",
      "| driver | no  | behind | 280 | 0.036663  |\n",
      "\n",
      "| M   | W      |   L |        Pr |\n",
      "|-----+--------+-----+-----------|\n",
      "| yes | no     | 280 | 0         |\n",
      "| yes | front  | 280 | 0         |\n",
      "| yes | behind | 280 | 0.0049995 |\n",
      "| no  | no     | 280 | 0         |\n",
      "| no  | front  | 280 | 0         |\n",
      "| no  | behind | 280 | 0.036663  |\n",
      "\n",
      "| W      | M   |   L |         Pr |\n",
      "|--------+-----+-----+------------|\n",
      "| no     | yes | 280 | 0          |\n",
      "| no     | no  | 280 | 0          |\n",
      "| front  | yes | 280 | 0          |\n",
      "| front  | no  | 280 | 0          |\n",
      "| behind | yes | 280 | 0.00166633 |\n",
      "| behind | no  | 280 | 0.0122198  |\n",
      "\n",
      "| M   |   L |         Pr |\n",
      "|-----+-----+------------|\n",
      "| yes | 280 | 0.00166633 |\n",
      "| no  | 280 | 0.0122198  |\n",
      "\n",
      "| M   |         Pr |\n",
      "|-----+------------|\n",
      "| yes | 0.00166633 |\n",
      "| no  | 0.0122198  |\n",
      "\n",
      "| S    | W      |   L | C      |       Pr |\n",
      "|------+--------+-----+--------+----------|\n",
      "| no   | no     | 260 | 3W     | 0        |\n",
      "| no   | no     | 260 | driver | 0        |\n",
      "| no   | front  | 260 | 3W     | 0        |\n",
      "| no   | front  | 260 | driver | 0        |\n",
      "| no   | behind | 260 | 3W     | 0        |\n",
      "| no   | behind | 260 | driver | 0.083325 |\n",
      "| down | no     | 260 | 3W     | 0        |\n",
      "| down | no     | 260 | driver | 0.083325 |\n",
      "| down | front  | 260 | 3W     | 0        |\n",
      "| down | front  | 260 | driver | 0        |\n",
      "| down | behind | 260 | 3W     | 0.083325 |\n",
      "| down | behind | 260 | driver | 0.16665  |\n",
      "| up   | no     | 260 | 3W     | 0        |\n",
      "| up   | no     | 260 | driver | 0        |\n",
      "| up   | front  | 260 | 3W     | 0        |\n",
      "| up   | front  | 260 | driver | 0        |\n",
      "| up   | behind | 260 | 3W     | 0        |\n",
      "| up   | behind | 260 | driver | 0        |\n",
      "\n",
      "| W      |   L | C      |       Pr |\n",
      "|--------+-----+--------+----------|\n",
      "| no     | 260 | 3W     | 0        |\n",
      "| no     | 260 | driver | 0.083325 |\n",
      "| front  | 260 | 3W     | 0        |\n",
      "| front  | 260 | driver | 0        |\n",
      "| behind | 260 | 3W     | 0.083325 |\n",
      "| behind | 260 | driver | 0.249975 |\n",
      "\n",
      "| C      | M   | W      |   Pr |\n",
      "|--------+-----+--------+------|\n",
      "| 3W     | yes | no     | 0.01 |\n",
      "| 3W     | yes | front  | 0.02 |\n",
      "| 3W     | yes | behind | 0.02 |\n",
      "| 3W     | no  | no     | 0.49 |\n",
      "| 3W     | no  | front  | 0.48 |\n",
      "| 3W     | no  | behind | 0.48 |\n",
      "| driver | yes | no     | 0.03 |\n",
      "| driver | yes | front  | 0.06 |\n",
      "| driver | yes | behind | 0.06 |\n",
      "| driver | no  | no     | 0.47 |\n",
      "| driver | no  | front  | 0.44 |\n",
      "| driver | no  | behind | 0.44 |\n",
      "\n",
      "| C      | M   | W      |   L |         Pr |\n",
      "|--------+-----+--------+-----+------------|\n",
      "| 3W     | yes | no     | 260 | 0          |\n",
      "| 3W     | yes | front  | 260 | 0          |\n",
      "| 3W     | yes | behind | 260 | 0.0016665  |\n",
      "| 3W     | no  | no     | 260 | 0          |\n",
      "| 3W     | no  | front  | 260 | 0          |\n",
      "| 3W     | no  | behind | 260 | 0.039996   |\n",
      "| driver | yes | no     | 260 | 0.00249975 |\n",
      "| driver | yes | front  | 260 | 0          |\n",
      "| driver | yes | behind | 260 | 0.0149985  |\n",
      "| driver | no  | no     | 260 | 0.0391627  |\n",
      "| driver | no  | front  | 260 | 0          |\n",
      "| driver | no  | behind | 260 | 0.109989   |\n",
      "\n",
      "| M   | W      |   L |         Pr |\n",
      "|-----+--------+-----+------------|\n",
      "| yes | no     | 260 | 0.00249975 |\n",
      "| yes | front  | 260 | 0          |\n",
      "| yes | behind | 260 | 0.016665   |\n",
      "| no  | no     | 260 | 0.0391627  |\n",
      "| no  | front  | 260 | 0          |\n",
      "| no  | behind | 260 | 0.149985   |\n",
      "\n",
      "| W      | M   |   L |          Pr |\n",
      "|--------+-----+-----+-------------|\n",
      "| no     | yes | 260 | 0.000833167 |\n",
      "| no     | no  | 260 | 0.0130529   |\n",
      "| front  | yes | 260 | 0           |\n",
      "| front  | no  | 260 | 0           |\n",
      "| behind | yes | 260 | 0.00555444  |\n",
      "| behind | no  | 260 | 0.04999     |\n",
      "\n",
      "| M   |   L |         Pr |\n",
      "|-----+-----+------------|\n",
      "| yes | 260 | 0.00638761 |\n",
      "| no  | 260 | 0.0630429  |\n",
      "\n",
      "| M   |         Pr |\n",
      "|-----+------------|\n",
      "| yes | 0.00638761 |\n",
      "| no  | 0.0630429  |\n",
      "\n",
      "P(M=yes|L>=260)= 0.09666666666666665\n"
     ]
    }
   ],
   "source": [
    "#### Your answer for 3\n",
    "\n",
    "from collections import OrderedDict as odict\n",
    "from itertools import product, combinations, permutations\n",
    "from tabulate import tabulate\n",
    "\n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "    if entry in factor['table']:\n",
    "        return factor['table'][entry]     # insert your code here, 1 line   \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). 1 line\n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. 1 line\n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. 1 line\n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "            \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. 1 line\n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s. 1 line\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    \n",
    "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
    "    table = list()\n",
    "    \n",
    "    # Here is where the magic happens. The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = tuple((entryDict[var] for var in f1['dom']))\n",
    "        f2_entry = tuple((entryDict[var] for var in f2['dom']))\n",
    "                \n",
    "        # Insert your code here\n",
    "        p1 = prob(f1, *f1_entry)           # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry \n",
    "        p2 = prob(f2, *f2_entry)           # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry \n",
    "        \n",
    "     \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "# Answer\n",
    "\n",
    "def VE(factors, order, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors, each factor is a dictionary of domain and probability values,\n",
    "    `order`, a list of variable names specifying an elimination order,\n",
    "    `outcomeSpace`, a dictionary with variable names and respective domains.\n",
    "    Returns a dictionary with non-eliminated factors\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of factors, so we can freely modify it without distroying the original dictionary\n",
    "    f = factors.copy()\n",
    "    # We process the factor in elimination order  \n",
    "    for i, var in enumerate(order):\n",
    "        # This is the domain of the new factor. We use sets as it is handy to eliminate duplicate variables\n",
    "        newFactorDom = set()\n",
    "        # This is a list of factors that will be removed from f because they were joined with other factors\n",
    "        listFactorsRemove = list()\n",
    "        # This is a flag to indicate if we are processing the first factor\n",
    "        first = True\n",
    "        # Lets iterate over all factors\n",
    "        for f_id in f.keys():\n",
    "            # and select the ones that have the variable to be eliminated\n",
    "            if var in f[f_id]['dom']:        \n",
    "                if first:\n",
    "                    # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "                    fx = f[f_id]\n",
    "                    first = False\n",
    "                else:\n",
    "                    # Join fx and f[f_id] and save the result in fx\n",
    "                    fx = join(fx, f[f_id], outcomeSpace)\n",
    "                    printFactor(fx)\n",
    "                    print()\n",
    "                # f_id was joined, so we will need to eliminate it from f later. Let's save that factor id for future removal\n",
    "                listFactorsRemove.append(f_id)\n",
    "        # Now, we need to remove var from the domain of the new factor doing a marginalization              \n",
    "        fx = marginalize(fx, var, outcomeSpace)\n",
    "        printFactor(fx)\n",
    "        print()\n",
    "    \n",
    "        # Now, we remove all factors that we joined. We do it outside the for loop since it modifies the data structure\n",
    "        for f_id in listFactorsRemove:\n",
    "            del f[f_id]\n",
    "        # We will create a new factor with id equal a sequential number and insert it into f, so it can be used in future joins          \n",
    "        f[i] = fx\n",
    "    return f\n",
    "\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        # Convert the tuple to a list to be able to manipulate it\n",
    "        k = list(key)\n",
    "        # Append the probability value to the list with key values\n",
    "        k.append(item)\n",
    "        # Append an entire row to the table\n",
    "        table.append(k)\n",
    "    # dom is used as table header. We need it converted to list\n",
    "    dom = list(f['dom'])\n",
    "    # Append a 'Pr' to indicate the probabity column\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='orgtbl'))\n",
    "    \n",
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      # Make a copy of outcomeSpace with a copy to method copy(). 1 line\n",
    "    newOutcomeSpace[var] = (e,)                # Replace the domain of variable var with a tuple with a single element e. 1 line\n",
    "    return newOutcomeSpace\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "def query(factors, order, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors\n",
    "    `order`, a list with variable elimination order\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor with P(Q, e) or P(Q|e)\n",
    "    \"\"\"     \n",
    "        \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    outSpace = outcomeSpace.copy()\n",
    "        \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)    \n",
    "    \n",
    "    for q_var in q_vars:\n",
    "        order.remove(q_var)\n",
    " \n",
    "    f = VE(factors, order, outSpace)\n",
    "        \n",
    "    first = True\n",
    "    for f_id in f.keys():\n",
    "        if first:\n",
    "            # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "            fx = f[f_id]\n",
    "            first = False\n",
    "        else:\n",
    "            # Join fx and f[f_id] and save the result in fx\n",
    "            fx = join(fx, f[f_id], outSpace)    \n",
    "    \n",
    "    return fx\n",
    "\n",
    "outcomeSpace = dict(\n",
    "    C=('3W','driver'),\n",
    "    W=('no','front', 'behind'),\n",
    "    M=('yes','no'),\n",
    "    S=('no', 'down', 'up'),\n",
    "    L=('140','160', '180', '200', '220', '240', '260', '280'),\n",
    ")\n",
    "\n",
    "factors = {\n",
    "    'C': {\n",
    "        'dom': ('C',), \n",
    "        'table': odict([\n",
    "            (('3W',), .5),\n",
    "            (('driver',), .5),\n",
    "        ])\n",
    "    }, \n",
    "\n",
    "    'W': {\n",
    "        'dom': ('W',), \n",
    "        'table': odict([\n",
    "            (('no',), .3333),\n",
    "            (('front',), .3333),\n",
    "            (('behind',), .3333),            \n",
    "        ])\n",
    "    }, \n",
    "\n",
    "    'S': {\n",
    "        'dom': ('S',), \n",
    "        'table': odict([\n",
    "            (('no',), .3333),\n",
    "            (('down',), .3333),\n",
    "            (('up',), .3333),            \n",
    "        ])\n",
    "    },     \n",
    "    \n",
    "    'M': {\n",
    "        'dom': ('C', 'W', 'M'),\n",
    "        'table': odict([\n",
    "            (('3W', 'no', 'yes'), .02),\n",
    "            (('3W', 'no', 'no'), .98),\n",
    "            (('3W', 'front', 'yes'), .04), \n",
    "            (('3W', 'front', 'no'), .96), \n",
    "            (('3W', 'behind', 'yes'), .04), \n",
    "            (('3W', 'behind', 'no'), .96), \n",
    "            (('driver', 'no', 'yes'), .06), \n",
    "            (('driver', 'no', 'no'), .94), \n",
    "            (('driver', 'front', 'yes'), .12), \n",
    "            (('driver', 'front', 'no'), .88), \n",
    "            (('driver', 'behind', 'yes'), .12), \n",
    "            (('driver', 'behind', 'no'), .88), \n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'L': {\n",
    "        'dom': ('C', 'W', 'S', 'L'),\n",
    "        'table': odict([\n",
    "            (('3W', 'no', 'no', '180'), .25), \n",
    "            (('3W', 'no', 'no', '200'), .50), \n",
    "            (('3W', 'no', 'no', '220'), .25), \n",
    "            (('3W', 'no', 'down', '200'), .25), \n",
    "            (('3W', 'no', 'down', '220'), .50), \n",
    "            (('3W', 'no', 'down', '240'), .25),\n",
    "            (('3W', 'no', 'up', '160'), .25), \n",
    "            (('3W', 'no', 'up', '180'), .50), \n",
    "            (('3W', 'no', 'up', '200'), .25),            \n",
    "            (('3W', 'front', 'no', '160'), .25), \n",
    "            (('3W', 'front', 'no', '180'), .50), \n",
    "            (('3W', 'front', 'no', '200'), .25), \n",
    "            (('3W', 'front', 'down', '180'), .25), \n",
    "            (('3W', 'front', 'down', '200'), .50), \n",
    "            (('3W', 'front', 'down', '220'), .25),\n",
    "            (('3W', 'front', 'up', '140'), .25), \n",
    "            (('3W', 'front', 'up', '160'), .50), \n",
    "            (('3W', 'front', 'up', '180'), .25),             \n",
    "            (('3W', 'behind', 'no', '200'), .25), \n",
    "            (('3W', 'behind', 'no', '220'), .50), \n",
    "            (('3W', 'behind', 'no', '240'), .25), \n",
    "            (('3W', 'behind', 'down', '220'), .25), \n",
    "            (('3W', 'behind', 'down', '240'), .50), \n",
    "            (('3W', 'behind', 'down', '260'), .25), \n",
    "            (('3W', 'behind', 'up', '180'), .25), \n",
    "            (('3W', 'behind', 'up', '200'), .50), \n",
    "            (('3W', 'behind', 'up', '220'), .25),             \n",
    "            (('driver', 'no', 'no', '200'), .25), \n",
    "            (('driver', 'no', 'no', '220'), .50), \n",
    "            (('driver', 'no', 'no', '240'), .25), \n",
    "            (('driver', 'no', 'down', '220'), .25), \n",
    "            (('driver', 'no', 'down', '240'), .50), \n",
    "            (('driver', 'no', 'down', '260'), .25), \n",
    "            (('driver', 'no', 'up', '180'), .25), \n",
    "            (('driver', 'no', 'up', '200'), .50), \n",
    "            (('driver', 'no', 'up', '220'), .25),             \n",
    "            (('driver', 'front', 'no', '180'), .25), \n",
    "            (('driver', 'front', 'no', '200'), .50), \n",
    "            (('driver', 'front', 'no', '220'), .25), \n",
    "            (('driver', 'front', 'down', '200'), .25), \n",
    "            (('driver', 'front', 'down', '220'), .50), \n",
    "            (('driver', 'front', 'down', '240'), .25), \n",
    "            (('driver', 'front', 'up', '160'), .25), \n",
    "            (('driver', 'front', 'up', '180'), .50), \n",
    "            (('driver', 'front', 'up', '200'), .25),             \n",
    "            (('driver', 'behind', 'no', '220'), .25), \n",
    "            (('driver', 'behind', 'no', '240'), .50), \n",
    "            (('driver', 'behind', 'no', '260'), .25), \n",
    "            (('driver', 'behind', 'down', '240'), .25), \n",
    "            (('driver', 'behind', 'down', '260'), .50), \n",
    "            (('driver', 'behind', 'down', '280'), .25), \n",
    "            (('driver', 'behind', 'up', '200'), .25), \n",
    "            (('driver', 'behind', 'up', '220'), .50), \n",
    "            (('driver', 'behind', 'up', '240'), .25),\n",
    "        ])\n",
    "    },\n",
    "}\n",
    "\n",
    "L280 = query(factors, ['S', 'C', 'W', 'M', 'L'], outcomeSpace, ('M',), L='280')\n",
    "L260 = query(factors, ['S', 'C', 'W', 'M', 'L'], outcomeSpace, ('M',), L='260')\n",
    "\n",
    "print(\"P(M=yes|L>=260)=\",(L260['table'][('yes',)]+L280['table'][('yes',)])/(L260['table'][('yes',)]+L280['table'][('yes',)]+L260['table'][('no',)]+L280['table'][('no',)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 7 [15 marks]\n",
    "\n",
    "In Lecture 15, we discussed techniques to learn parameters from data, and it became evident that learning in the presence of missing data is significantly more expensive than with complete data. In this question, we ask you to analyse the time complexity of the EM algorithm in more detail.\n",
    "\n",
    "1. [**10 marks**] The Expectation Maximisation (EM) algorithm requires inference on the Bayesian network. Explain how the jointree algorithm can help to improve the running time of the EM algorithm. In particular, queries of the form $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ involve setting evidence $\\textbf{d}_i$ for every training example $i$. How does it impact the performance of the jointree algorithm in terms of new messages that need to be exchanged?\n",
    "2. [**5 marks**] It is a common practice to use *random restarts* with EM since the algorithm is sensitive to the initial parameter estimate $\\theta^0$. The idea is to run the algorithm multiple times and return the best estimate. Explain how you would implement random restarts with EM. How would you initialise the algorithm, and how would you select the best estimates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 1\n",
    "\n",
    "The jointree algorithm will allow computing marginals for clusters $C_i$. As noted in the slide 32, these clusters involve family marginals of the form $x\\textbf{u}$ that also appear in the EM queries. Unfortunately, we need to set evidence for each example that will require exchanging new messages. We can expect to exchange most of the jointree messages for the examples that are complete or almost complete. Therefore, a call to exchange messages for the jointree algorithm needs to be placed between the **for** loops in the algorithm below. That is certainly very expensive. We will need to update the jointree for each example. On the other hand, the jointree will allow us to compute all the queries $P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$. There are several of those queries, one for each family in the network.\n",
    "\n",
    "For completeness, this is the algorithm of slide 31 modified to incorporate inference with a jointree. \n",
    "\n",
    "$J \\leftarrow$ create_jointree($G$) { create a jointree from the $G$ }<br/>\n",
    "distribute_factors($G$, $J$) { distribute factors of $G$ among the clusters of $J$ }<br/>\n",
    "$k \\leftarrow 0$<br/>\n",
    "$\\theta^k \\leftarrow$ initial parameter values<br/>\n",
    "**while** convergence criterion is not met **do**<br/>\n",
    "$~~~~$ $c_{x\\textbf{u}} \\leftarrow 0$ for each family instantiation $x\\textbf{u}$<br/>\n",
    "$~~~~$ **for** $i \\leftarrow 1$ to $N$ **do**<br/>\n",
    "$~~~~$$~~~~$ set_evidence($J, \\textbf{d}_i)$ { set evidence in jointree $J$ according to $\\textbf{d}_i$ }<br/>\n",
    "$~~~~$$~~~~$ exchange_messages($J, r$) { exchange messages in jointree $J$ according to root node $r$ }<br/>\n",
    "$~~~~$$~~~~$ **for** each family instantiation $x\\textbf{u}$ **do**<br/>\n",
    "$~~~~$$~~~~$$~~~~$ $c_{x\\textbf{u}} \\leftarrow c_{x\\textbf{u}} + P_{\\theta^k}(x\\textbf{u}|\\textbf{d}_i)$ {requires inference on network $(G,\\theta^k)$}<br/>\n",
    "$~~~~$ $\\theta_{x|\\textbf{u}}^{k+1} \\leftarrow c_{x\\textbf{u}}/\\sum_{x^*}c_{x^*\\textbf{u}}$<br/>\n",
    "$~~~~$ $k \\leftarrow k + 1$<br/>\n",
    "**return** $\\theta^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer for 2\n",
    "\n",
    "We can start with random initializations, so we provide different start points for EM. Another possibility is to make random imputation of missing values and use as different start points. Missing data imputation is a procedure that replaces the missing values by estimates. In this case, the estimates will be random values sampled from the variable outcome space. This second technique has the benefit of providing initial values that naturally respect the probability distribution constraints.\n",
    "\n",
    "We select as the best estimate the one with the largest log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 3 [30 marks]\n",
    "\n",
    "Part 3 is composed of two programming questions of 15 marks each. Use the code cell after each question statement to enter your answer. You can use the code of the tutorials in your answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 8 [15 marks]\n",
    "\n",
    "Lecture 9 presented the concept of **separation**, an independence test for Markov networks. The idea is similar, but simpler than d-separation since Markov networks do not have \"convergent valves\". The test can be summarized in a single sentence:\n",
    "\n",
    "Let $\\textbf{X}$, $\\textbf{Y}$, and $\\textbf{Z}$ be three disjoint sets of nodes in a graph $G$. We say that $\\textbf{X}$ is separated from $\\textbf{Y}$ given $\\textbf{Z}$, written $sep_G(\\textbf{X},\\textbf{Z},\\textbf{Y})$, if and only if every path between a node in $\\textbf{X}$ and a node in $\\textbf{Y}$ pass through a node in $\\textbf{Z}$.\n",
    "\n",
    "An efficient separation test can be implemented by pruning the edges of $\\textbf{Z}$ and testing for connectivity between nodes in $\\textbf{X}$ and $\\textbf{Y}$.\n",
    "\n",
    "Implement an efficient separation test for Markov networks. The function `separation(G, X, Z, Y)` returns true if $\\textbf{X}$ is separated of $\\textbf{Y}$ given $\\textbf{Z}$ in the graph $G$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our answer for Question 8 here\n",
    "\n",
    "def dfs_r(G, x, Y, colour):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `v`, next vertex to be visited\n",
    "    `colour`, dictionary with the colour of each node\n",
    "    \"\"\"\n",
    "    if x in Y:\n",
    "        return True\n",
    "    colour[x] = 'grey'\n",
    "    for w in G[x]:\n",
    "        if colour[w] == 'white':\n",
    "            if dfs_r(G, w, Y, colour):\n",
    "                return True\n",
    "    colour[x] = 'black'\n",
    "    return False\n",
    "\n",
    "def connected(G, X, Y):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `start`, starting vertex\n",
    "    \"\"\"    \n",
    "    colour = {node: 'white' for node in G.keys()}\n",
    "    for x in X:\n",
    "        if colour[x] == 'white':\n",
    "            if dfs_r(G, x, Y, colour):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def separation(G, X, Z, Y):\n",
    "    G2 = G.copy()\n",
    "    for e in Z:\n",
    "        G2[e] = []\n",
    "    return not connected(G2, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time8 0.00024580955505371094\n",
      "Score8 15\n"
     ]
    }
   ],
   "source": [
    "# Test code Q8\n",
    "\n",
    "def time_penalty(t):\n",
    "    if t < 2:\n",
    "        return 0\n",
    "    elif t < 4:\n",
    "        return 1\n",
    "    elif t < 6:\n",
    "        return 2\n",
    "    elif t < 10:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v, []) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "\n",
    "    return GT\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "res = []\n",
    "\n",
    "g = {\n",
    "    'A': ['B'],\n",
    "    'B': ['A', 'C', 'D', 'F'],\n",
    "    'C': ['B'],\n",
    "    'D': ['B', 'F'],\n",
    "    'E': ['F'],\n",
    "    'F': ['B', 'D', 'E', 'G'],\n",
    "    'G': ['F'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "res.append(separation(g, ['A'], ['D'], ['G']))\n",
    "res.append(separation(g, ['A'], ['C'], ['G']))\n",
    "res.append(separation(g, ['A'], ['F'], ['G']))\n",
    "res.append(separation(g, ['A'], ['D', 'E'], ['G']))\n",
    "res.append(separation(g, ['A', 'D'], ['C', 'G'], ['E', 'F']))\n",
    "res.append(separation(g, ['A', 'D'], ['F'], ['E', 'G']))\n",
    "\n",
    "g = {\n",
    "    0:[1,8],\n",
    "    1:[2,0],\n",
    "    2:[3,1],\n",
    "    3:[4,2],\n",
    "    4:[5,3],\n",
    "    5:[6,4],\n",
    "    6:[7,5],\n",
    "    7:[8,6],\n",
    "    8:[0,7],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "res.append(separation(g, [0],[],[6]))\n",
    "res.append(separation(g, [0],[3,8],[6]))\n",
    "res.append(separation(g, [0],[3],[6]))\n",
    "res.append(separation(g, [0],[8],[6]))\n",
    "res.append(separation(g, [1,3,5],[8],[6,2]))\n",
    "res.append(separation(g, [1,3,5],[6,0],[7,8]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = {\n",
    "    0:[1,8],\n",
    "    1:[2,0],\n",
    "    2:[3,1,12],\n",
    "    3:[4,2],\n",
    "    4:[5,3],\n",
    "    5:[6,4],\n",
    "    6:[7,5],\n",
    "    7:[8,6],\n",
    "    8:[0,7,9,11],\n",
    "    9:[10,8],\n",
    "    10:[11,9],\n",
    "    11:[8,10],\n",
    "    12:[2]\n",
    "}\n",
    "\n",
    "\n",
    "res.append(separation(g, [0],[],[6]))\n",
    "res.append(separation(g, [0],[3,8],[6]))\n",
    "res.append(separation(g, [0],[3],[6]))\n",
    "res.append(separation(g, [0],[8],[6]))\n",
    "res.append(separation(g, [1,3,5],[8],[6,2]))\n",
    "res.append(separation(g, [1,3,5],[6,0],[7,8]))\n",
    "res.append(separation(g, [4,12,10],[2,5],[6,1]))\n",
    "res.append(separation(g, [4,12,10],[2,5,11,9],[6,1]))\n",
    "res.append(separation(g, [4,12,10],[2,5,9],[6,1]))\n",
    "res.append(separation(g, [9,1],[8,10],[11,7]))\n",
    "res.append(separation(g, [9,1],[10,8],[11,7]))\n",
    "res.append(separation(g, [9,1],[10,8,4],[11,7]))\n",
    "res.append(separation(g, [9,1],[10,8,4,12],[11,7]))\n",
    "res.append(separation(g, [9,1],[10,8,12],[11,7]))\n",
    "\n",
    "\n",
    "g = {\n",
    "    0:[1,8],\n",
    "    1:[2,0],\n",
    "    2:[3,1,12],\n",
    "    3:[4,2],\n",
    "    4:[5,3],\n",
    "    5:[6,4],\n",
    "    6:[7,5],\n",
    "    7:[8,6],\n",
    "    8:[0,7,9,11],\n",
    "    9:[10,8],\n",
    "    10:[11,9],\n",
    "    11:[8,10],\n",
    "    12:[2],\n",
    "    'A': ['B'],\n",
    "    'B': ['A', 'C', 'D', 'F'],\n",
    "    'C': ['B'],\n",
    "    'D': ['B', 'F'],\n",
    "    'E': ['F'],\n",
    "    'F': ['B', 'D', 'E', 'G'],\n",
    "    'G': ['F'],\n",
    "}\n",
    "\n",
    "\n",
    "res.append(separation(g, ['A'],[],[6]))\n",
    "res.append(separation(g, ['A'],[1,2,3,4,5,6,7,8,9],['C']))\n",
    "res.append(separation(g, ['A'],[],[8]))\n",
    "res.append(separation(g, ['A'],[],[6]))\n",
    "def make_grid(size):\n",
    "    g = {}\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            neighbours = []\n",
    "            for dx,dy in [(1,0),(0,1),(-1,0),(0,-1)]:\n",
    "                if x+dx >= 0 and x+dx < size and y+dy >=0 and y+dy <size:\n",
    "                    neighbours.append(str(x+dx)+str(y+dy))\n",
    "            g[str(x)+str(y)] = neighbours\n",
    "    return g\n",
    "                \n",
    "            \n",
    "g = make_grid(10)\n",
    "\n",
    "import time\n",
    "s = time.time()\n",
    "res.append(separation(g, ['01','50'],['03','12','22','32','41','52','61','70'],['45','05']))\n",
    "res.append(separation(g, ['01','50'],['03','12','22','32','41','52','61','40'],['45','05']))\n",
    "res.append(separation(g, ['01','50'],['03','12','22','32','41','52','61'],['45','05']))\n",
    "e = time.time()\n",
    "\n",
    "t8 = e-s\n",
    "print(\"Time8\", t8)\n",
    "testResults8 = np.array(res) == [False, False, True, False, False, True, False, True, False, False, False, True, False, True, False, False, False, True, False, True, False, False, False, True, True, False, True, False, True, True, True, False, False]\n",
    "score8 = max(0, 10*sum(testResults8)/len(testResults8))\n",
    "if score8 == 10:\n",
    "    # assign 15 marks if perfect score, otherwise assign proportion of test cases passed out of 10\n",
    "    score8 = 15\n",
    "score8 = max(0,score8-time_penalty(t8))\n",
    "print(\"Score8\",score8)\n",
    "# If functions/arguments are passed incorrectly, I removed 4 marks manually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 9 [15 marks]\n",
    "\n",
    "In Lecture 16, we discussed the importance of measuring model complexity. A common way to do so is to count the number of independent parameters in a Bayesian network.\n",
    "\n",
    "In this exercise, you will write a function `dimension(G, outcomeSpace)` which takes a DAG `G` and its associated `outcomeSpace`, and returns the dimension of `G`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our answer for Question 8 here\n",
    "\n",
    "def transposeGraph(G):\n",
    "    GT = dict((v, []) for v in G)\n",
    "    for v in G:\n",
    "        for w in G[v]:\n",
    "            GT[w].append(v)\n",
    "\n",
    "    return GT\n",
    "\n",
    "def dimension(G, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `G`, an adjacency list representation of a graph\n",
    "    `start`, starting vertex\n",
    "    \"\"\"\n",
    "    d = 0\n",
    "    GT = transposeGraph(G)\n",
    "    for x in GT:\n",
    "        dx = len(outcomeSpace[x])-1\n",
    "        for u in GT[x]:\n",
    "            dx *= len(outcomeSpace[u])\n",
    "        d += dx\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 9:\n",
      "Time9 0.008138656616210938\n",
      "Score9 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Question 9:\")\n",
    "res = []\n",
    "graph = {\n",
    "    'L': ['S', 'V'],\n",
    "    'H': ['S', 'V'],\n",
    "    'S': ['O'],\n",
    "    'V': ['C', 'O'],\n",
    "    'O': ['B'],\n",
    "    'A': ['T'],\n",
    "    'T': ['B'],\n",
    "    'C': [],\n",
    "    'B': [],\n",
    "}\n",
    "\n",
    "outSpace = dict(\n",
    "    H=(0,1),\n",
    "    L=(0,1),\n",
    "    A=(0,1),\n",
    "    V=(0,1),\n",
    "    S=(0,1),\n",
    "    T=(0,1),\n",
    "    C=(0,1,2),\n",
    "    O=(0,1,2),\n",
    "    B=(0,1,2),\n",
    ")\n",
    "\n",
    "res.append(dimension(graph, outSpace))\n",
    "graph = transposeGraph(graph)\n",
    "res.append(dimension(graph, outSpace))\n",
    "\n",
    "graph = {\n",
    "    'A': ['B'],\n",
    "    'B': ['A', 'C', 'D', 'F'],\n",
    "    'C': ['B'],\n",
    "    'D': ['B', 'F'],\n",
    "    'E': ['F'],\n",
    "    'F': ['B', 'D', 'E', 'G'],\n",
    "    'G': ['F'],\n",
    "}\n",
    "outSpace = {\n",
    "    'A': (0,1,2,3,4),\n",
    "    'B': (0,1,2),\n",
    "    'C': (1,3,4,5,6,7,8,9),\n",
    "    'D': ('a',2,3,4,5,6,7),\n",
    "    'E': (0,3,4,5,6,7,8,6,5,),\n",
    "    'F': (2,),\n",
    "    'G': (3,4),\n",
    "}\n",
    "\n",
    "res.append(dimension(graph, outSpace))\n",
    "graph = transposeGraph(graph)\n",
    "res.append(dimension(graph, outSpace))\n",
    "\n",
    "outSpace = {\n",
    "    'A': (0,1,2,3,4),\n",
    "    'B': (0,1,2),\n",
    "    'C': (1,3,4,5,6,7,8,9),\n",
    "    'D': ('a',2,5,6,7),\n",
    "    'E': (0,3,4,5,6,7,8,6,5,),\n",
    "    'F': (2,),\n",
    "    'G': (3,4),\n",
    "}\n",
    "\n",
    "res.append(dimension(graph, outSpace))\n",
    "graph = transposeGraph(graph)\n",
    "res.append(dimension(graph, outSpace))\n",
    "\n",
    "\n",
    "graph = {\n",
    "    'A': ['B'],\n",
    "    'B': ['C','D'],\n",
    "    'C': [],\n",
    "    'D': [],\n",
    "    'E': ['F','G'],\n",
    "    'F': [],\n",
    "    'G': [],\n",
    "}\n",
    "\n",
    "res.append(dimension(graph, outSpace))\n",
    "graph = transposeGraph(graph)\n",
    "res.append(dimension(graph, outSpace))\n",
    "\n",
    "graph = dict((str(i),[]) for i in range(10000))\n",
    "graph['0'] = [str(i) for i in range(1,10000)]\n",
    "outSpace = dict((str(i), tuple(j for j in range(1000))) for i in range(10000))\n",
    "res.append(dimension(graph, outSpace))\n",
    "import time\n",
    "s = time.time()\n",
    "res.append(dimension(graph, outSpace))\n",
    "e = time.time()\n",
    "t9 = e-s\n",
    "print(\"Time9\", t9)\n",
    "\n",
    "testResults9 = np.array(res) == [37, 35, 620, 620, 454, 454, 64, 120, 9989001999, 9989001999]\n",
    "\n",
    "score9 = max(0, 10*sum(testResults9)/len(testResults9))\n",
    "if score9 == 10:\n",
    "    # assign 15 marks if perfect score, otherwise assign proportion of test cases passed out of 10\n",
    "    score9 = 15\n",
    "score9 = max(0,score9-time_penalty(t9))\n",
    "print(\"Score9\",score9)\n",
    "# If functions/arguments are passed incorrectly, I removed 4 marks manually\n",
    "# E.g. relying on a global variable called \"outcomeSpace\", instead of the one that was passed to the function\n",
    "# If the code raised an exception, the score was decided based on the severity of the mistake. Sometimes a try-except statement was added to the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "783px",
    "left": "0px",
    "right": "1346.87px",
    "top": "108px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
