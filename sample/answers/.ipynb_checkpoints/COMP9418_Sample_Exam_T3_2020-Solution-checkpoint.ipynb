{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Sample Exam Solution\n",
    "\n",
    "**COMP9418 - Advanced Topics in Statistical Machine Learning**\n",
    "\n",
    "**University of New South Wales**\n",
    "\n",
    "**7th December, 2020**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing with reading this, please read and acknowledge the following (double-click on this cell and put an `X` between the brackets `[X]`:\n",
    "    \n",
    "- [ ] <span style=\"color:red\">I acknowledge that I will complete all of the work I submit for this exam without assistance from anyone else.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instructions\n",
    "\n",
    "1. This exam will last for 24 hours, starting 7/12/2020 at 00:00:01 AEST and ending 7/12/2020 at 23:59:59 AEST.\n",
    "2. Questions will be answered from 9 am to 9 pm AEST. Questions should be posted in the  [WebCMS forum](https://webcms3.cse.unsw.edu.au/COMP9418/20T3/forums/) or sent by email to cs9418@cse.unsw.edu.au.\n",
    "3. All answers must be provided in this Jupyter notebook. \n",
    "4. You must use the cells provided to answer the questions. Use markdown cells for textual answers and code cells for programming answers.\n",
    "5. Submit this exam by give (command line or WebCMS) before the deadline. No late submissions will be accepted.\n",
    "6. The exam will have three parts: Multiple choice questions (20%). Questions that require a textual answer (50%). Programming questions in Python (30%).\n",
    "7. This exam is an open book exam. You are permitted to access papers and books as well as the course materials, including slides and solved tutorials.\n",
    "8. You are not permitted to communicate (email, phone, message, talk, etc.) with anyone during the exam, except COMP9418 staff via email or forum.\n",
    "9. Deliberate violation of exam conditions will be referred to Student Integrity as serious misconduct.\n",
    "10. This exam has eight questions. The total number of marks is 100.\n",
    "11. Type your student number and name on the next cell.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student identification\n",
    "\n",
    "**Name:** <span style=\"color:red\">Jon Snow</span>\n",
    "\n",
    "**Student ID:** <span style=\"color:red\">z1234567</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 [20 marks]\n",
    "\n",
    "Part 1 is composed of four multiple-choice questions of five marks each. To answer each question, double-click the cell with the alternatives and write an `X` between the `[ ]` of the chosen option.\n",
    "\n",
    "This is an example before inserting `X`\n",
    "\n",
    "1. [ ] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "This is an example after inserting `X`\n",
    "\n",
    "1. [X] Alternative one\n",
    "2. [ ] Alternative two\n",
    "\n",
    "For all four questions, choose only one among the five alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 1 [5 marks]\n",
    "\n",
    "Suppose you have an extensive Bayesian network and you want to use it to do classification. In summary, you have a query node $Q$ representing a discrete variable with a domain size of $|Q|$. Your queries are MPE instantiation queries in the form:\n",
    "\n",
    "$$argmax_q P(Q=q|X_1=x_{i1}, X_2=x_{i2}, \\ldots, X_n=x_{in})$$\n",
    "\n",
    "where $X_1, \\ldots, X_n$ are the remaining variables in the network and $x_{i1}, \\ldots, x_{in}$ are the values corresponding to the test example $i$ for the variables $X_1, \\ldots, X_n$, respectively. Assume that your test set is complete, i.e., each test example has values for all network variables but the query $Q$.\n",
    "\n",
    "Due to the size of the network, you need to speed up this computation. Because of all variables but $Q$ are instantiated, you think you can reduce the query to:\n",
    "\n",
    "$$argmax_q P(Q=q|S(\\textbf{X})=\\textbf{x}_i^s))$$\n",
    "\n",
    "where $S(\\textbf{X})$ is a subset of variables of the network and $\\textbf{x}_i^s$ is a subset of values from example $i$ that matches the variables in $S(\\textbf{X})$.\n",
    "\n",
    "Choose the correct alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] $S(\\textbf{X})$ is the set of parent nodes of $Q$. When we set evidence for all parents of a node $Q$, the CPT of $Q$ reduces to $|Q|$ entries (for instance, 2 if $Q$ is a binary variable). The entry with the highest probability is the answer for the MPE query.\n",
    "2. [ ] $S(\\textbf{X})$ is the set of parent and child nodes of $Q$. When we set evidence for all parents and children of a node, we d-separate this node from the remaining of the network.\n",
    "3. [X] $S(\\textbf{X})$ is the set of parents, children and spouses of $Q$. Therefore $S(\\textbf{X})$ corresponds to the Markov blanket of $Q$.\n",
    "4. [ ] We cannot predefine $S(\\textbf{X})$ for given node $Q$ and a general Bayesian network. We need to run the d-separation algorithm and verify which nodes are independent of $Q$.\n",
    "5. [ ] We have to run an inference algorithm since independence is a dynamic concept. Independent variables can become dependent, given evidence and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 2 [5 marks]\n",
    "\n",
    "Choose the **incorrect** alternative regarding the differences between Belief Propagation (BP), Iterative Belief Propagation (IBP - also known as Loopy Belief Propagation) and Generalized Belief Propagation (GBP - also known as Iterative Joingraph Propagation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] BP is restricted to polytrees while IBP and GBP can be applied to general networks.\n",
    "2. [ ] In BP, there is always a root or leaf node with a single neighbour that we can use to start to send messages. In IBP and GPB, such nodes may not exist. However, we initialize all messages with a value, so all nodes are allowed to send an initial message.\n",
    "3. [ ] BP provides exact results, that is, the computed marginals (beliefs) are correct. IBP and GBP do not have the same guarantees, and the beliefs are usually approximations of the true marginals.\n",
    "4. [ ] BP converges in a single iteration. IBP and GBP need a convergence criterium, so these methods iterate until such criterium is met.\n",
    "5. [X] When sending a message from a node *X* to a node *Y*, BP *X*'s message is the multiplication of incoming messages from all neighbours of *X* (including *Y*) by the factors associated with *X*. In the case of evidence, it also multiplies by the evidence indicator associated with *X*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Question 3 [5 marks]\n",
    "\n",
    "Suppose you use a jointree algorithm to make inference. However, the jointree $J$ with the lowest width does not have clusters that include all variables that will be present in the queries. For instance, you may want to compute $P(\\textbf{X})$ for a set of variables $\\textbf{X}$. However, there is no cluster $\\textbf{C}_i$ such that $\\textbf{X} \\subseteq \\textbf{C}_i$.\n",
    "\n",
    "![Jointree question 4](img/Question3.png)\n",
    "\n",
    "For the jointree above, you may be interested in the query $P(GC)$. Therefore $\\textbf{X} = GC$. \n",
    "\n",
    "Select the option with the **correct** alternative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [ ] You can select a set of $k$ clusters, say, $\\textbf{C}_1, ..., \\textbf{C}_k$ such that $\\textbf{X} \\subseteq \\textbf{C}_1 \\cup, ..., \\cup \\textbf{C}_k$. To answer the queries, we join the marginals computed in $J$ for these clusters and eliminate the variables not present in $\\textbf{X}$. In the example above, you could join the marginals for $FDG$ and $ACE$ and remove variables $F, D, A$ and $E$.\n",
    "2. [ ] You can manipulate $J$ by adding to a chosen cluster $\\textbf{C}_i$ the variables in the set $\\textbf{X} \\cap \\textbf{C}_i$. No further modification of $J$ is needed. You compute the messages after manipulating $J$. In the example above, you could add the variable $C$ to the cluster $FDG$.\n",
    "3. [ ] You can manipulate $J$ by merging a cluster $\\textbf{C}_i$ with other clusters of $J$ such that the resulting cluster has all variables in $X$. No further modification of $J$ is needed. You compute the messages after manipulating $J$. In the example above, you could replace cluster $FDG$ with $FDG \\cup ACE$.\n",
    "4. [X] You can manipulate $J$ by adding to a chosen cluster $\\textbf{C}_i$ the variables in the set $\\textbf{X} \\cap \\textbf{C}_i$. However, this modification may incur in modifications to other clusters to keep the running intersection property of $J$.\n",
    "5. [ ] None of the above alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4 [5 marks]\n",
    "\n",
    "Which one of the following schemes converts a Bayesian network structure $G$ to a valid joingraph $U$? Choose the correct alternative. (Remember that $X$'s family is a set of nodes that includes $X$ and its parents if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [X] For each node $X_i$ in $G$, define a cluster $\\textbf{C}_i$ over $X_i$â€™s family. Connect $\\textbf{C}_i$ and $\\textbf{C}_j$ if $X_j$ is a parent of $X_i$ in $G$, and define the separator to be the $\\{X_j\\}$.\n",
    "2. [ ] For each node $X_i$ in $G$, define a cluster $\\textbf{C}_i$ over $X_i$â€™s family. Connect $\\textbf{C}_i$ and $\\textbf{C}_j$ if $X_j$ is a parent of $X_i$ in $G$, and define the separator to be the intersection of the clusters.\n",
    "3. [ ] For each node $X_i$ in $G$, define a cluster $\\textbf{C}_i$ over $X_i$â€™s family. Connect $\\textbf{C}_i$ and $\\textbf{C}_j$ if $X_j$ is a parent of $X_i$ in $G$, and define the separator to be the $\\{X_i\\}$.\n",
    "4. [ ] For each node $X_i$ in $G$, define a cluster $\\textbf{C}_i$ over $X_i$â€™s family. Connect $\\textbf{C}_i$ and $\\textbf{C}_j$ if $X_j$ is a parent of $X_i$ in $G$, and define the separator to be the $\\{X_i,X_j\\}$.\n",
    "5. [ ] None of the above alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 2 [50 marks]\n",
    "\n",
    "Part 2 is composed of two open questions of 25 marks each. To answer each question, edit the markdown cell after the question statement and insert your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5 [25 marks]\n",
    "\n",
    "Jack has three coins $C_1$, $C_2$ and $C_3$ with $p_1$, $p_2$ and $p_3$ as their corresponding probabilities of landing heads. Jack flips coin $C_1$ twice and then decides, based on the outcome, whether to flip coin $C_2$ or $C_3$ next. In particular, if the two $C_1$ flips come out the same, Jack flips coin $C_2$ three times next. However, if the $C_1$ flips come out different, he flips coin $C_3$ three times next. Given the outcome of Jack's last three flips, we want to know whether his first two flips came out the same. \n",
    "\n",
    "1. [**5 Marks**] Show a Bayesian network structure (graph) for this problem. **Briefly** explain your network.\n",
    "2. [**5 Marks**] Show the network conditional probability tables (CPTs) for all variables. If you have parameters that are shared among variables, define the CPT once and indicate which variables use that CPT.\n",
    "3. [**5 Marks**] Provide a query that solves this problem.\n",
    "4. [**10 Marks**] What is the solution to this problem assuming that $p_1 = 0.4, p_2 = 0.6$ and $p_3 = 0.1$, and the last three flips came out as follows (show your work. This item can be solved as a simulation exercise -- you provide a step-by-step solution -- or a programming exercise -- you provide a program that computes a step-by-step solution)\n",
    "    1. tails, heads, tails\n",
    "    2. tails, tails, tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"216pt\" height=\"192pt\"\n",
       " viewBox=\"0.00 0.00 216.13 191.52\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 187.5201)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-187.5201 212.1291,-187.5201 212.1291,4 -4,4\"/>\n",
       "<!-- C11 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>C11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"57.5777\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.5777\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">C11</text>\n",
       "</g>\n",
       "<!-- Cx -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Cx</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"104.5622\" cy=\"-84.2245\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.5622\" y=\"-80.5245\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Cx</text>\n",
       "</g>\n",
       "<!-- C11&#45;&gt;Cx -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>C11&#45;&gt;Cx</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.1919,-34.3701C74.5164,-41.875 80.9457,-50.9371 86.8544,-59.2654\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.1144,-61.4521 92.7553,-67.5827 89.8235,-57.4016 84.1144,-61.4521\"/>\n",
       "</g>\n",
       "<!-- C12 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>C12</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"153.7139\" cy=\"-19.6153\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"153.7139\" y=\"-15.9153\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">C12</text>\n",
       "</g>\n",
       "<!-- C12&#45;&gt;Cx -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>C12&#45;&gt;Cx</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M141.3109,-35.9189C135.7386,-43.2436 129.0493,-52.0365 122.9141,-60.1013\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"120.0602,-58.072 116.791,-68.1499 125.6313,-62.3103 120.0602,-58.072\"/>\n",
       "</g>\n",
       "<!-- O1 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>O1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"102.9244\" cy=\"-165.5201\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.9244\" y=\"-161.8201\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">O1</text>\n",
       "</g>\n",
       "<!-- Cx&#45;&gt;O1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Cx&#45;&gt;O1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M104.1987,-102.2666C103.9924,-112.5095 103.7295,-125.5606 103.4961,-137.1443\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"99.9948,-137.1788 103.2926,-147.2473 106.9934,-137.3199 99.9948,-137.1788\"/>\n",
       "</g>\n",
       "<!-- O2 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>O2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"181.1291\" cy=\"-111.4062\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"181.1291\" y=\"-107.7062\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">O2</text>\n",
       "</g>\n",
       "<!-- Cx&#45;&gt;O2 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Cx&#45;&gt;O2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M128.7885,-92.8249C134.7113,-94.9276 141.1554,-97.2153 147.4326,-99.4437\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"146.4657,-102.8145 157.0605,-102.8617 148.8076,-96.2178 146.4657,-102.8145\"/>\n",
       "</g>\n",
       "<!-- O3 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>O3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"27\" cy=\"-108.5133\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-104.8133\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">O3</text>\n",
       "</g>\n",
       "<!-- Cx&#45;&gt;O3 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Cx&#45;&gt;O3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M80.021,-91.9096C74.0795,-93.7702 67.6201,-95.793 61.3198,-97.7659\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"60.1448,-94.4662 51.6477,-100.7948 62.2367,-101.1463 60.1448,-94.4662\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8ee760bbe0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer for item 1 - graph\n",
    "\n",
    "# Define your graph here\n",
    "graph = {\n",
    "    'C11': ['Cx'],\n",
    "    'C12': ['Cx'],\n",
    "    'Cx': ['O1', 'O2', 'O3'],\n",
    "    'O1': [],\n",
    "    'O2': [],\n",
    "    'O3': [],\n",
    "}\n",
    "\n",
    "# Do not modify this code, it simply plots the graph above\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(engine=\"neato\", comment='Direct graph example')\n",
    "dot.attr(overlap=\"false\", splines=\"true\")\n",
    "\n",
    "for v in graph.keys():\n",
    "    dot.node(v)\n",
    "    # dot.node(str(v))              # position the nodes in random order\n",
    "\n",
    "for v in graph.keys():\n",
    "    for w in graph[v]:\n",
    "        dot.edge(v, w)\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** \n",
    "\n",
    "`C11` and `C12` represent the outcome of the first two throws of coin $C1$. `Cx` is the XOR of these variables and will tell us if they are the same or not. `O1`, `O2` and `O3` are the three final outcomes. They depend on `Cx` outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**\n",
    "\n",
    "\n",
    "Variables $C_{11}$ and $C_{12}$ have the following CPT\n",
    "\n",
    "| $C_i$  | $P(C_i)$  |\n",
    "| :-:   |    :-:   |\n",
    "| heads |    $p_1$ |\n",
    "| tails |  $1-p_1$ |\n",
    "\n",
    "Variable $Cx$ has the following CPT\n",
    "\n",
    "| $C_{11}$ | $C_{12}$ | $Cx$  | $P(Cx | C_{11},C_{12})$ |\n",
    "|  :-:   |  :-:   |  :-  | :-: |\n",
    "| heads  | heads  | 0     | <img width=100/>1   |\n",
    "| heads  | heads  | 1     | 0   |\n",
    "| heads  | tails  | 0     | 0   |\n",
    "| heads  | tails  | 1     | 1   |\n",
    "| tails  | heads  | 0     | 0   |\n",
    "| tails  | heads  | 1     | 1   |\n",
    "| tails  | tails  | 0     | 1   |\n",
    "| tails  | tails  | 1     | 0   |\n",
    "\n",
    "Variables $O_{1}$, $O_{2}$, and $O_{3}$ have the following CPT\n",
    "\n",
    "| $Cx$  | $O_i$   | $P(O_i|Cx)$  |\n",
    "| :-   | :-     |    :-:       |\n",
    "| 0     | heads   |   $p2$       |\n",
    "| 0     | tails   | $1-p_2$      |\n",
    "| 1     | heads   | $p_3$        |\n",
    "| 1     | tails   | $1-p_3$      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**\n",
    "\n",
    "$P(Cx | O_1 = o_1, O_2 = o_2, O_3 = o_3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Cx | O1    | O2    | O3    |        Pr |\n",
      "|------+-------+-------+-------+-----------|\n",
      "|    0 | tails | tails | tails | 0.0868476 |\n",
      "|    1 | tails | tails | tails | 0.913152  |\n",
      "\n",
      "|   Cx | O1    | O2    | O3    |       Pr |\n",
      "|------+-------+-------+-------+----------|\n",
      "|    0 | tails | heads | tails | 0.562162 |\n",
      "|    1 | tails | heads | tails | 0.437838 |\n"
     ]
    }
   ],
   "source": [
    "# Item 4 as programming exercise\n",
    "\n",
    "from collections import OrderedDict as odict\n",
    "from itertools import product, combinations, permutations\n",
    "from tabulate import tabulate\n",
    "\n",
    "outcomeSpace = dict(\n",
    "    C11=('heads','tails'),\n",
    "    C12=('heads','tails'),\n",
    "    O1=('heads','tails'),\n",
    "    O2=('heads','tails'),\n",
    "    O3=('heads','tails'),    \n",
    "    Cx=(0,1)\n",
    ")\n",
    "\n",
    "factors = {\n",
    "    'C11': {\n",
    "        'dom': ('C11',), \n",
    "        'table': odict([\n",
    "            (('heads',), 0.4),\n",
    "            (('tails',), 0.6),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'C12': {\n",
    "        'dom': ('C12',), \n",
    "        'table': odict([\n",
    "            (('heads',), 0.4),\n",
    "            (('tails',), 0.6),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'Cx': {\n",
    "        'dom': ('C11', 'C12', 'Cx'), \n",
    "        'table': odict([\n",
    "            (('heads', 'heads', 0), 1),\n",
    "            (('heads', 'heads', 1), 0),\n",
    "            (('heads', 'tails', 0), 0),\n",
    "            (('heads', 'tails', 1), 1),\n",
    "            (('tails', 'heads', 0), 0),\n",
    "            (('tails', 'heads', 1), 1),\n",
    "            (('tails', 'tails', 0), 1),\n",
    "            (('tails', 'tails', 1), 0),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'O1': {\n",
    "        'dom': ('Cx', 'O1'), \n",
    "        'table': odict([\n",
    "            ((0, 'heads'), .6),\n",
    "            ((0, 'tails'), .4),\n",
    "            ((1, 'heads'), .1),\n",
    "            ((1, 'tails'), .9),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'O2': {\n",
    "        'dom': ('Cx', 'O2'), \n",
    "        'table': odict([\n",
    "            ((0, 'heads'), .6),\n",
    "            ((0, 'tails'), .4),\n",
    "            ((1, 'heads'), .1),\n",
    "            ((1, 'tails'), .9),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'O3': {\n",
    "        'dom': ('Cx', 'O3'), \n",
    "        'table': odict([\n",
    "            ((0, 'heads'), .6),\n",
    "            ((0, 'tails'), .4),\n",
    "            ((1, 'heads'), .1),\n",
    "            ((1, 'tails'), .9),\n",
    "        ])\n",
    "    },    \n",
    "}\n",
    "\n",
    "def prob(factor, *entry):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factor`, a dictionary of domain and probability values,\n",
    "    `entry`, a list of values, one for each variable in the same order as specified in the factor domain.\n",
    "    \n",
    "    Returns p(entry)\n",
    "    \"\"\"\n",
    "\n",
    "    return factor['table'][entry]     # insert your code here, 1 line   \n",
    "\n",
    "def marginalize(f, var, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be marginalized.\n",
    "    `var`, variable to be summed out.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
    "    new_dom = list(f['dom'])\n",
    "    new_dom.remove(var)            # Remove var from the list new_dom by calling the method remove(). 1 line\n",
    "    table = list()                 # Create an empty list for table. We will fill in table from scratch. 1 line\n",
    "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
    "        s = 0;                     # Initialize the summation variable s. 1 line\n",
    "\n",
    "        # We need to iterate over all possible outcomes of the variable var\n",
    "        for val in outcomeSpace[var]:\n",
    "            # To modify the tuple entries, we will need to convert it to a list\n",
    "            entriesList = list(entries)\n",
    "            # We need to insert the value of var in the right position in entriesList\n",
    "            entriesList.insert(f['dom'].index(var), val)\n",
    "            \n",
    "            p = prob(f, *tuple(entriesList))     # Calculate the probability of factor f for entriesList. 1 line\n",
    "            s = s + p                            # Sum over all values of var by accumulating the sum in s. 1 line\n",
    "            \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, s))\n",
    "    return {'dom': tuple(new_dom), 'table': odict(table)}\n",
    "\n",
    "def join(f1, f2, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f1`, first factor to be joined.\n",
    "    `f2`, second factor to be joined.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns a new factor with a join of f1 and f2\n",
    "    \"\"\"\n",
    "    \n",
    "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
    "    # But it is important to eliminate the repetitions\n",
    "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
    "    \n",
    "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
    "    table = list()\n",
    "    \n",
    "    # Here is where the magic happens. The product iterator will generate all combinations of varible values \n",
    "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
    "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
    "        \n",
    "        # We need to map the entries to the domain of the factors f1 and f2\n",
    "        entryDict = dict(zip(common_vars, entries))\n",
    "        f1_entry = tuple((entryDict[var] for var in f1['dom']))\n",
    "        f2_entry = tuple((entryDict[var] for var in f2['dom']))\n",
    "                \n",
    "        # Insert your code here\n",
    "        p1 = prob(f1, *f1_entry)           # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry \n",
    "        p2 = prob(f2, *f2_entry)           # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry \n",
    "        \n",
    "     \n",
    "        \n",
    "        # Create a new table entry with the multiplication of p1 and p2\n",
    "        table.append((entries, p1 * p2))\n",
    "\n",
    "    return {'dom': tuple(common_vars), 'table': odict(table)}\n",
    "\n",
    "# Answer\n",
    "\n",
    "def VE(factors, order, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors, each factor is a dictionary of domain and probability values,\n",
    "    `order`, a list of variable names specifying an elimination order,\n",
    "    `outcomeSpace`, a dictionary with variable names and respective domains.\n",
    "    Returns a dictionary with non-eliminated factors\n",
    "    \"\"\"    \n",
    "\n",
    "    # Let's make a copy of factors, so we can freely modify it without distroying the original dictionary\n",
    "    f = factors.copy()\n",
    "    # We process the factor in elimination order  \n",
    "    for i, var in enumerate(order):\n",
    "        # This is the domain of the new factor. We use sets as it is handy to eliminate duplicate variables\n",
    "        newFactorDom = set()\n",
    "        # This is a list of factors that will be removed from f because they were joined with other factors\n",
    "        listFactorsRemove = list()\n",
    "        # This is a flag to indicate if we are processing the first factor\n",
    "        first = True\n",
    "        # Lets iterate over all factors\n",
    "        for f_id in f.keys():\n",
    "            # and select the ones that have the variable to be eliminated\n",
    "            if var in f[f_id]['dom']:        \n",
    "                if first:\n",
    "                    # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "                    fx = f[f_id]\n",
    "                    first = False\n",
    "                else:\n",
    "                    # Join fx and f[f_id] and save the result in fx\n",
    "                    fx = join(fx, f[f_id], outcomeSpace)\n",
    "                    #printFactor(fx)\n",
    "                # f_id was joined, so we will need to eliminate it from f later. Let's save that factor id for future removal\n",
    "                listFactorsRemove.append(f_id)\n",
    "        # Now, we need to remove var from the domain of the new factor doing a marginalization              \n",
    "        fx = marginalize(fx, var, outcomeSpace)\n",
    "        #printFactor(fx)\n",
    "    \n",
    "        # Now, we remove all factors that we joined. We do it outside the for loop since it modifies the data structure\n",
    "        for f_id in listFactorsRemove:\n",
    "            del f[f_id]\n",
    "        # We will create a new factor with id equal a sequential number and insert it into f, so it can be used in future joins          \n",
    "        f[i] = fx\n",
    "    return f\n",
    "\n",
    "def printFactor(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, a factor to print on screen\n",
    "    \"\"\"\n",
    "    # Create a empty list that we will fill in with the probability table entries\n",
    "    table = list()\n",
    "    \n",
    "    # Iterate over all keys and probability values in the table\n",
    "    for key, item in f['table'].items():\n",
    "        # Convert the tuple to a list to be able to manipulate it\n",
    "        k = list(key)\n",
    "        # Append the probability value to the list with key values\n",
    "        k.append(item)\n",
    "        # Append an entire row to the table\n",
    "        table.append(k)\n",
    "    # dom is used as table header. We need it converted to list\n",
    "    dom = list(f['dom'])\n",
    "    # Append a 'Pr' to indicate the probabity column\n",
    "    dom.append('Pr')\n",
    "    print(tabulate(table,headers=dom,tablefmt='orgtbl'))\n",
    "    \n",
    "def evidence(var, e, outcomeSpace):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `var`, a valid variable identifier.\n",
    "    `e`, the observed value for var.\n",
    "    `outcomeSpace`, dictionary with the domain of each variable\n",
    "    \n",
    "    Returns dictionary with a copy of outcomeSpace with var = e\n",
    "    \"\"\"    \n",
    "    newOutcomeSpace = outcomeSpace.copy()      # Make a copy of outcomeSpace with a copy to method copy(). 1 line\n",
    "    newOutcomeSpace[var] = (e,)                # Replace the domain of variable var with a tuple with a single element e. 1 line\n",
    "    return newOutcomeSpace\n",
    "\n",
    "def normalize(f):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `f`, factor to be normalized.\n",
    "    \n",
    "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
    "    \"\"\" \n",
    "    table = list()\n",
    "    sum = 0\n",
    "    for k, p in f['table'].items():\n",
    "        sum = sum + p\n",
    "    for k, p in f['table'].items():\n",
    "        table.append((k, p/sum))\n",
    "    return {'dom': f['dom'], 'table': odict(table)}\n",
    "\n",
    "#Answer\n",
    "\n",
    "def query(factors, order, outcomeSpace, q_vars, **q_evi):\n",
    "    \"\"\"\n",
    "    argument \n",
    "    `factors`, a dictionary of factors\n",
    "    `order`, a list with variable elimination order\n",
    "    `outcomeSpace`, dictionary will variable domains\n",
    "    `q_vars`, list of variables in query head\n",
    "    `q_evi`, dictionary of evidence in the form of variables names and values\n",
    "    \n",
    "    Returns a new factor with P(Q, e) or P(Q|e)\n",
    "    \"\"\"     \n",
    "        \n",
    "    # Let's make a copy of these structures, since we will reuse the variable names\n",
    "    outSpace = outcomeSpace.copy()\n",
    "        \n",
    "    # First, we set the evidence \n",
    "    for var_evi, e in q_evi.items():\n",
    "        outSpace = evidence(var_evi, e, outSpace)    \n",
    "    \n",
    "    for q_var in q_vars:\n",
    "        order.remove(q_var)\n",
    " \n",
    "    f = VE(factors, order, outSpace)\n",
    "        \n",
    "    first = True\n",
    "    for f_id in f.keys():\n",
    "        if first:\n",
    "            # We need this code since join requires two factors, so we save the first one in fx and wait for the next\n",
    "            fx = f[f_id]\n",
    "            first = False\n",
    "        else:\n",
    "            # Join fx and f[f_id] and save the result in fx\n",
    "            fx = join(fx, f[f_id], outSpace)    \n",
    "    \n",
    "    #printFactor(fx)\n",
    "    return normalize(fx)\n",
    "\n",
    "printFactor(query(factors, ['C11', 'C12', 'Cx'], outcomeSpace, ('Cx',), O1='tails',O2='tails',O3='tails'))\n",
    "print()\n",
    "printFactor(query(factors, ['C11', 'C12', 'Cx'], outcomeSpace, ('Cx',), O1='tails',O2='heads',O3='tails'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 6 [25 marks]\n",
    "\n",
    "Markov chains were subject of two lectures: 8 and 14. Lecture 8 defined the chain and their properties such as regularity and peridiocidity as well as the concept of stationary distribution. Lecture 14 covered Markov Chain Monte Carlo methods such as Gibbs sampling. However, these techniques seem disconnected since Gibbs sampling is an approximate inference algorithm, while Markov chains are stochastic models. In this question, we ask you to elucidate the connections between these two methods.\n",
    "\n",
    "Remember that Markov chains are characterized by the transition probabilities between states, frequently organized in the form of a transition matrix. Answer **briefly**\n",
    "\n",
    "1. [**5 marks**] What is a state for the Gibbs chain? \n",
    "2. [**5 marks**] What is the transition matrix for the Gibbs chain? How big is this matrix? What are the transition probabilities?\n",
    "3. [**5 marks**] What is the stationary distribution for the Gibbs chain? What is its importance for sampling and its connection to mixing?\n",
    "4. [**10 marks**] What makes Gibbs sampling suitable for making inference on Markov networks compared to forward and rejection sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A state is a complete assignment for the variables in the Bayesian/Markov network. Complete assignment means that all variables receive a value from their domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The transition table for the Gibbs chain is potentially large table. Given a network with $n$ variables with domain size $d$, there are $d^n$ possible states. Therefore, such a table has size $(d^n)^2$. The table is sparse, since most transition probabilities are zero. Gibbs sampling only allows transition between states that differ on most one variable since we can only change one variable at a time.\n",
    "\n",
    "The transition probabilities $P(\\textbf{X}_t=\\textbf{x}'|\\textbf{X}_{t-1}=\\textbf{x})$ are:\n",
    "\n",
    "1. 0 if $\\textbf{x}$ and $\\textbf{x'}$ differ on more than one variable.\n",
    "2. $\\frac{1}{m}P(s'|\\textbf{x}-S)$ if $\\textbf{x}$ and $\\textbf{x'}$ differ on a single variable $S$, which has value $s'$ in $\\textbf{x}'$.\n",
    "3. $\\frac{1}{m}\\sum_{S\\in\\textbf{X}}P(s_{\\textbf{x}}|\\textbf{x}-S)$ if $\\textbf{x} = \\textbf{x'}$ and $s_{\\textbf{x}}$ is the value of variable $S$ in $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The stationary distribution for the Gibbs chain is network distribution $P(\\textbf{X})$ or $P(\\textbf{X}|\\textbf{e})$ if evidence $\\textbf{e}$ is observed. Therefore, when we sample from the stationary distribution we will be sampling from the network distribution. However, we need to guarantee that the Gibbs chain has mixed since we typically start from a random state that is far from the $P$. Unfortunately, there is no widely accepted test for mixing. Most tests accept that the chain has mixed when a good amount of evidence has been collected and none of it indicates that the chain has not mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The fact that Gibbs sampling works with complete assignments. There is no need to specify an order for sampling the variables. Such an order is natural for Bayesian networks, but not for Markov networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Part 3 [30 marks]\n",
    "\n",
    "Part 3 is composed of two programming questions of 15 marks each. Use the code cell after each question statement to enter your answer. You can use the code of the tutorials in your answers. You may import any libraries that were used in the tutorials, but no others.\n",
    "\n",
    "Load the next cell. The Bayesian network example will be used in both exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict as odict\n",
    "from math import log\n",
    "# x = log(y, 2) to compute log y in base 2\n",
    "\n",
    "graph = {\n",
    "    'A': ['B'],\n",
    "    'B': ['E'],\n",
    "    'C': ['E', 'F'],\n",
    "    'D': ['G'],\n",
    "    'E': ['G', 'H'],\n",
    "    'F': [],\n",
    "    'G': ['I'],\n",
    "    'H': [],\n",
    "    'I': [],    \n",
    "}\n",
    "\n",
    "outcomeSpace = dict(\n",
    "    A=(0,1),\n",
    "    B=(0,1),\n",
    "    C=(0,1),\n",
    "    D=(0,1),\n",
    "    E=(0,1),\n",
    "    F=(0,1),\n",
    "    G=(0,1),\n",
    "    H=(0,1),\n",
    "    I=(0,1),\n",
    ")\n",
    "\n",
    "factors = {\n",
    "    'A': {\n",
    "        'dom': ('A',), \n",
    "        'table': odict([\n",
    "            ((0,), 0.80),\n",
    "            ((1,), 0.20),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'B' : {\n",
    "        'dom': ('A', 'B'),\n",
    "        'table': odict([\n",
    "            ((0, 0), 0.95),\n",
    "            ((0, 1), 0.05),\n",
    "            ((1, 0), 0.80),\n",
    "            ((1, 1), 0.20),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'C': {\n",
    "        'dom': ('C',), \n",
    "        'table': odict([\n",
    "            ((0,), 0.50),\n",
    "            ((1,), 0.50),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'D': {\n",
    "        'dom': ('D',), \n",
    "        'table': odict([\n",
    "            ((0,), 0.40),\n",
    "            ((1,), 0.60),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'E': {\n",
    "        'dom': ('B', 'C', 'E'), \n",
    "        'table': odict([\n",
    "            ((0, 0, 0), 0.05),\n",
    "            ((0, 0, 1), 0.95),\n",
    "            ((0, 1, 0), 0.99),\n",
    "            ((0, 1, 1), 0.01),\n",
    "            ((1, 0, 0), 0.80),\n",
    "            ((1, 0, 1), 0.20),\n",
    "            ((1, 1, 0), 0.50),\n",
    "            ((1, 1, 1), 0.50),\n",
    "        ])\n",
    "    },\n",
    "\n",
    "    'F' : {\n",
    "        'dom': ('C', 'F'),\n",
    "        'table': odict([\n",
    "            ((0, 0), 0.10),\n",
    "            ((0, 1), 0.90),\n",
    "            ((1, 0), 0.40),\n",
    "            ((1, 1), 0.60),\n",
    "        ])\n",
    "    },    \n",
    "\n",
    "    'G': {\n",
    "        'dom': ('D', 'E', 'G'), \n",
    "        'table': odict([\n",
    "            ((0, 0, 0), 0.15),\n",
    "            ((0, 0, 1), 0.85),\n",
    "            ((0, 1, 0), 0.70),\n",
    "            ((0, 1, 1), 0.30),\n",
    "            ((1, 0, 0), 0.80),\n",
    "            ((1, 0, 1), 0.20),\n",
    "            ((1, 1, 0), 0.25),\n",
    "            ((1, 1, 1), 0.75),\n",
    "        ])\n",
    "    },\n",
    "    \n",
    "    'H' : {\n",
    "        'dom': ('E', 'H'),\n",
    "        'table': odict([\n",
    "            ((0, 0), 0.50),\n",
    "            ((0, 1), 0.50),\n",
    "            ((1, 0), 0.45),\n",
    "            ((1, 1), 0.55),\n",
    "        ])\n",
    "    }, \n",
    "    \n",
    "    'I' : {\n",
    "        'dom': ('G', 'I'),\n",
    "        'table': odict([\n",
    "            ((0, 0), 0.60),\n",
    "            ((0, 1), 0.40),\n",
    "            ((1, 0), 0.55),\n",
    "            ((1, 1), 0.45),\n",
    "        ])\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = [\n",
    "    dict(A=1,B=1,C=0,D=1,E=0,F=1,G=1,H=1,I=1),\n",
    "    dict(A=0,B=1,C=1,D=1,E=1,F=1,G=1,H=0,I=0),\n",
    "    dict(A=1,B=0,C=1,D=0,E=0,F=0,G=0,H=1,I=0),\n",
    "    dict(A=1,B=0,C=0,D=1,E=0,F=0,G=0,H=0,I=1),\n",
    "    dict(A=0,B=1,C=0,D=1,E=0,F=1,G=1,H=1,I=1),\n",
    "    dict(A=0,B=0,C=1,D=1,E=1,F=1,G=0,H=0,I=1),\n",
    "    dict(A=1,B=1,C=1,D=0,E=1,F=0,G=1,H=1,I=0),\n",
    "    dict(A=0,B=1,C=0,D=1,E=1,F=1,G=0,H=0,I=1),\n",
    "    dict(A=0,B=0,C=0,D=0,E=1,F=0,G=1,H=1,I=0),\n",
    "    dict(A=0,B=0,C=0,D=1,E=1,F=0,G=0,H=0,I=0),  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 7 [15 marks]\n",
    "\n",
    "In Question 1 of this exam, we asked you how we can speed up MPE queries in the case we have observed all variables but the query variable. In this question, you have to provide a Python function that implements such an efficient inference algorithm.\n",
    "\n",
    "Implement a function `MPE_classify(G, factors, outcomeSpace, q_var, q_evi)` that returns the most likely instantiation of the variable `q_var` given the evidence list `q_evi` for a graph `G` with a factor dictionary `factors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Write our answer here\n",
    "\n",
    "def MPE_classify(G, factors, outcomeSpace, q_var, q_evi):\n",
    "    children = G[q_var]\n",
    "    mpe = {key: 1 for key in outcomeSpace[q_var]}\n",
    "    relevantFactors = children + [q_var]\n",
    "    for i in outcomeSpace[q_var]:\n",
    "        mpe[i] = 1\n",
    "        q_evi[q_var] = i\n",
    "        for j in relevantFactors:\n",
    "            evi = tuple([q_evi[x] for d in factors[j]['dom'] for x in d])\n",
    "            mpe[i] = mpe[i] * factors[j]['table'][evi]\n",
    "    return max(mpe, key=mpe.get)\n",
    "\n",
    "\n",
    "############\n",
    "# Test code\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    print(MPE_classify(graph, factors, outcomeSpace, 'E', dataset[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 8 [15 marks]\n",
    "\n",
    "A central component of learning structures from data is the computation of the log-likelihood functions for a Bayesian network $G$ and a dataset $D$. An important result is that such log-likelihood corresponds to a sum of terms, one for each family in the network:\n",
    "\n",
    "$$LL(G;D)=-N \\sum_{X\\textbf{U}} H_D(X|\\textbf{U})$$\n",
    "\n",
    "where $H_D(X|\\textbf{U})$ is the conditional entropy for the family $X\\textbf{U}$.\n",
    "\n",
    "Implement a Python function `LL(factors, outcomeSpace, dataset)` that computes the log-likelihood of a graph $G$ defined over a dictionary of factors `factors` using the dataset `dataset`. Define an intermediate function `entropy(factors, X, outcomeSpace, dataset)` that computes the conditional entropy for the family of variable `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-77.59291128035022\n"
     ]
    }
   ],
   "source": [
    "# Write your answer here\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def entropy(Gt, X, outcomeSpace, dataset):\n",
    "    e = 0\n",
    "    for entries in product(*[outcomeSpace[node] for node in Gt[X]]):\n",
    "        for x in outcomeSpace[X]:\n",
    "            c_u = 0\n",
    "            c_x_u = 0\n",
    "            for i in range(len(dataset)):\n",
    "                if all(dataset[i][Gt[X][j]] == entries[j] for j in range(len(Gt[X]))):\n",
    "                    c_u += 1\n",
    "                    if dataset[i][X] == x:\n",
    "                        c_x_u += 1\n",
    "            if c_x_u > 0:\n",
    "                p_x__u = c_x_u / c_u\n",
    "                p_x_u = c_x_u / len(dataset)\n",
    "                e += p_x_u * log(p_x__u, 2)\n",
    "    return -e\n",
    "\n",
    "def LL(G, outcomeSpace, dataset):\n",
    "    ll = 0\n",
    "    Gt = transposeGraph(G)\n",
    "    for var in Gt.keys():\n",
    "        ll += entropy(Gt, var, outcomeSpace, dataset)\n",
    "    return -ll * len(dataset)\n",
    "\n",
    "\n",
    "############\n",
    "# Test code\n",
    "\n",
    "print(LL(graph, outcomeSpace, dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "783px",
    "left": "0px",
    "right": "1346.87px",
    "top": "108px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
